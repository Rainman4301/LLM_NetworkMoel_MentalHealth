{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e9a0e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\MDS1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.metrics import silhouette_score\n",
    "import random\n",
    "from itertools import product\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "def topic_modeling(platform, label, original_cols, stopwords, min_df=1, max_df=0.9,\n",
    "                   n_neighbors=15, min_cluster_size=30, min_samples=10, nr_topics='auto',\n",
    "                   use_pretrained_embed=True, grid_search=False, n_samples=100):\n",
    "    \n",
    "    # Set paths\n",
    "    cur_dir = os.getcwd()\n",
    "    target_path = os.path.join(cur_dir, 'data', f'{platform}_data', 'berttopic_label', f'{label}')\n",
    "    label_df = pd.read_csv(os.path.join(target_path, \"label_df.csv\"))\n",
    "\n",
    "    # Load documents\n",
    "    with open(os.path.join(target_path, 'unlemma_dc'), 'r', encoding='utf-8') as f:\n",
    "        unlemma_dc = [line.strip() for line in f if line.strip()]\n",
    "    if not unlemma_dc:\n",
    "        print(f\"No valid documents remain after preprocessing for label '{label}'.\")\n",
    "\n",
    "    with open(os.path.join(target_path, 'lemma_dc'), 'r', encoding='utf-8') as f:\n",
    "        lemma_dc = [line.strip() for line in f if line.strip()]\n",
    "    if not lemma_dc:\n",
    "        print(f\"No lemmatized documents found for label '{label}'.\")\n",
    "\n",
    "    # Load embeddings\n",
    "    embedding = np.load(os.path.join(target_path, 'embedding.npy'))\n",
    "    if embedding.size == 0 or len(embedding) != len(lemma_dc):\n",
    "        print(f\"Embedding generation failed or mismatched for label '{label}': {len(embedding)} embeddings, {len(lemma_dc)} documents.\")\n",
    "    reduced_embedding = np.load(os.path.join(target_path, 'reduce_embedding.npy'))\n",
    "    if reduced_embedding.size == 0:\n",
    "        print(f\"Embedding generation failed for label '{label}'.\")\n",
    "\n",
    "    # Custom function to filter n-grams from vocabulary\n",
    "    def filter_ngrams_vocabulary(vectorizer, documents, unwanted_ngrams):\n",
    "        vectorizer.fit(documents)\n",
    "        vocab = vectorizer.get_feature_names_out()\n",
    "        filtered_vocab = [term for term in vocab if term not in unwanted_ngrams]\n",
    "        new_vectorizer = CountVectorizer(\n",
    "            ngram_range=vectorizer.ngram_range,\n",
    "            stop_words=vectorizer.stop_words,\n",
    "            min_df=vectorizer.min_df,\n",
    "            max_df=vectorizer.max_df,\n",
    "            vocabulary=filtered_vocab\n",
    "        )\n",
    "        return new_vectorizer\n",
    "\n",
    "    # Evaluation functions\n",
    "    def extract_topic_words(topics_dict, topk: int = 10):\n",
    "        topics_clean = []\n",
    "        for tid, pairs in topics_dict.items():\n",
    "            if tid == -1:\n",
    "                continue\n",
    "            topic_words = []\n",
    "            for word, _ in pairs[:topk]:\n",
    "                split_words = word.strip().split()\n",
    "                topic_words.extend(split_words)\n",
    "            if topic_words:\n",
    "                topics_clean.append(list(dict.fromkeys(topic_words)))\n",
    "        return topics_clean\n",
    "\n",
    "    def topic_coherence(topics_list, docs, topk: int = 10):\n",
    "        dictionary = Dictionary(doc.split() for doc in docs)\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topics_list,\n",
    "            texts=[doc.split() for doc in docs],\n",
    "            dictionary=dictionary,\n",
    "            coherence=\"c_v\",\n",
    "            topn=topk,\n",
    "        )\n",
    "        return coherence_model.get_coherence()\n",
    "\n",
    "    def topic_diversity(topics_dict, topk: int = 10):\n",
    "        all_words = [w for _, pairs in topics_dict.items() if _ != -1\n",
    "                     for w, _ in pairs[:topk]]\n",
    "        return len(set(all_words)) / (len(topics_dict) * topk)\n",
    "\n",
    "    def topic_silhouette(embeddings, topics_labels):\n",
    "        valid_idx = [i for i, t in enumerate(topics_labels) if t != -1]\n",
    "        X_valid = embeddings[valid_idx]\n",
    "        y_valid = np.array(topics_labels)[valid_idx]\n",
    "        if len(np.unique(y_valid)) < 2:\n",
    "            return 0.0\n",
    "        return silhouette_score(X_valid, y_valid, metric=\"cosine\")\n",
    "\n",
    "    # Initialize SentenceTransformer model\n",
    "    sentence_transformer_model = models.Transformer(\n",
    "        model_name_or_path=\"mental/mental-bert-base-uncased\",\n",
    "        tokenizer_name_or_path=\"mental/mental-bert-base-uncased\",\n",
    "        max_seq_length=512\n",
    "    )\n",
    "    pooling_model = models.Pooling(\n",
    "        sentence_transformer_model.get_word_embedding_dimension(),\n",
    "        pooling_mode_mean_tokens=True\n",
    "    )\n",
    "    mentalbert_sentence_model = SentenceTransformer(device=\"cuda\", modules=[sentence_transformer_model, pooling_model])\n",
    "\n",
    "    # Grid search logic\n",
    "    if grid_search:\n",
    "        # Define hyperparameter ranges\n",
    "        max_df_range = np.linspace(0.4, 0.95, num=12)  # 12 values between 0.4 and 0.95\n",
    "        n_neighbors_range = np.arange(5, 36, 5)  # 5 to 35, step 5\n",
    "        min_samples_range = np.arange(5, 41, 5)  # 5 to 40, step 5\n",
    "\n",
    "        # Generate random sample of parameter combinations\n",
    "        param_combinations = list(product(max_df_range, n_neighbors_range, min_samples_range))\n",
    "        if n_samples > len(param_combinations):\n",
    "            n_samples = len(param_combinations)\n",
    "        sampled_combinations = random.sample(param_combinations, n_samples)\n",
    "\n",
    "        results = []\n",
    "        for idx, (max_df_val, n_neighbors_val, min_samples_val) in enumerate(sampled_combinations):\n",
    "            print(f\"Testing combination {idx+1}/{n_samples}: max_df={max_df_val:.3f}, n_neighbors={n_neighbors_val}, min_samples={min_samples_val}\")\n",
    "\n",
    "            # Initialize models with current parameters\n",
    "            vectorizer_model = CountVectorizer(\n",
    "                ngram_range=(1, 3),\n",
    "                stop_words='english',\n",
    "                min_df=min_df,\n",
    "                max_df=max_df_val\n",
    "            )\n",
    "            vectorizer_model = filter_ngrams_vocabulary(vectorizer_model, lemma_dc, stopwords)\n",
    "\n",
    "            umap_model = UMAP(\n",
    "                n_neighbors=n_neighbors_val,\n",
    "                n_components=5,\n",
    "                min_dist=0.0,\n",
    "                metric='cosine',\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            hdbscan_model = HDBSCAN(\n",
    "                min_cluster_size=min_cluster_size,\n",
    "                min_samples=min_samples_val,\n",
    "                metric='euclidean',\n",
    "                prediction_data=True\n",
    "            )\n",
    "\n",
    "            ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "            representation_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "            # Initialize and fit BERTopic\n",
    "            topic_model = BERTopic(\n",
    "                embedding_model=mentalbert_sentence_model,\n",
    "                umap_model=umap_model,\n",
    "                hdbscan_model=hdbscan_model,\n",
    "                vectorizer_model=vectorizer_model,\n",
    "                ctfidf_model=ctfidf_model,\n",
    "                representation_model=representation_model,\n",
    "                top_n_words=10,\n",
    "                nr_topics=nr_topics,\n",
    "                calculate_probabilities=True\n",
    "            )\n",
    "\n",
    "            # Fit model\n",
    "            if use_pretrained_embed:\n",
    "                topics, probs = topic_model.fit_transform(documents=lemma_dc, embeddings=embedding)\n",
    "            else:\n",
    "                topics, probs = topic_model.fit_transform(documents=lemma_dc)\n",
    "\n",
    "            # Reduce outliers\n",
    "            num_outliers = np.sum(np.array(topics) == -1)\n",
    "            if num_outliers > 0:\n",
    "                new_topics = topic_model.reduce_outliers(\n",
    "                    documents=lemma_dc,\n",
    "                    topics=topics,\n",
    "                    probabilities=probs,\n",
    "                    strategy=\"probabilities\",\n",
    "                    threshold=0.6\n",
    "                )\n",
    "                if num_outliers != new_topics.count(-1):\n",
    "                    topic_model.update_topics(lemma_dc, topics=new_topics, vectorizer_model=vectorizer_model,\n",
    "                                              ctfidf_model=ctfidf_model, representation_model=representation_model)\n",
    "\n",
    "            # Evaluate\n",
    "            topics_dict = topic_model.get_topics()\n",
    "            topics_labels = topics\n",
    "            topics_list = extract_topic_words(topics_dict, topk=10)\n",
    "\n",
    "            coh = topic_coherence(topics_list, lemma_dc, topk=10)\n",
    "            div = topic_diversity(topics_dict, topk=10)\n",
    "            sil = topic_silhouette(embedding, topics_labels)\n",
    "\n",
    "            # Store results\n",
    "            results.append({\n",
    "                'max_df': max_df_val,\n",
    "                'n_neighbors': n_neighbors_val,\n",
    "                'min_samples': min_samples_val,\n",
    "                'coherence': coh,\n",
    "                'diversity': div,\n",
    "                'silhouette': sil,\n",
    "                'combined_score': (coh + div + sil) / 3  # Simple average for combined score\n",
    "            })\n",
    "\n",
    "        # Sort results by combined score and select top 5\n",
    "        results = sorted(results, key=lambda x: x['combined_score'], reverse=True)\n",
    "        top_5_results = results[:5]\n",
    "\n",
    "        print(\"\\nTop 5 Parameter Combinations:\")\n",
    "        for i, res in enumerate(top_5_results, 1):\n",
    "            print(f\"\\nRank {i}:\")\n",
    "            print(f\"max_df: {res['max_df']:.3f}, n_neighbors: {res['n_neighbors']}, min_samples: {res['min_samples']}\")\n",
    "            print(f\"Coherence: {res['coherence']:.4f}, Diversity: {res['diversity']:.4f}, Silhouette: {res['silhouette']:.4f}\")\n",
    "            print(f\"Combined Score: {res['combined_score']:.4f}\")\n",
    "\n",
    "        # Save results to CSV\n",
    "        results_df = pd.DataFrame(results)\n",
    "        output_path = os.path.join(cur_dir, f'topic_modeling_results_{label.replace(\" \", \"_\").lower()}.csv')\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nGrid search results saved to: {output_path}\")\n",
    "\n",
    "        return top_5_results\n",
    "\n",
    "    else:\n",
    "        # Original code for single run\n",
    "        vectorizer_model = CountVectorizer(\n",
    "            ngram_range=(1, 3),\n",
    "            stop_words='english',\n",
    "            min_df=min_df,\n",
    "            max_df=max_df\n",
    "        )\n",
    "        vectorizer_model = filter_ngrams_vocabulary(vectorizer_model, lemma_dc, stopwords)\n",
    "\n",
    "        umap_model = UMAP(\n",
    "            n_neighbors=n_neighbors,\n",
    "            n_components=5,\n",
    "            min_dist=0.0,\n",
    "            metric='cosine',\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        hdbscan_model = HDBSCAN(\n",
    "            min_cluster_size=min_cluster_size,\n",
    "            min_samples=min_samples,\n",
    "            metric='euclidean',\n",
    "            prediction_data=True\n",
    "        )\n",
    "\n",
    "        ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "        representation_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=mentalbert_sentence_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            ctfidf_model=ctfidf_model,\n",
    "            representation_model=representation_model,\n",
    "            top_n_words=10,\n",
    "            nr_topics=nr_topics,\n",
    "            calculate_probabilities=True\n",
    "        )\n",
    "\n",
    "        if use_pretrained_embed:\n",
    "            topics, probs = topic_model.fit_transform(documents=lemma_dc, embeddings=embedding)\n",
    "        else:\n",
    "            topics, probs = topic_model.fit_transform(documents=lemma_dc)\n",
    "\n",
    "        num_outliers = np.sum(np.array(topics) == -1)\n",
    "        if num_outliers > 0:\n",
    "            new_topics = topic_model.reduce_outliers(\n",
    "                documents=lemma_dc,\n",
    "                topics=topics,\n",
    "                probabilities=probs,\n",
    "                strategy=\"probabilities\",\n",
    "                threshold=0.6\n",
    "            )\n",
    "            print(f\"Before Number of outliers: {num_outliers}\")\n",
    "            print(f\"After Number of outliers: {new_topics.count(-1)}\")\n",
    "            if num_outliers != new_topics.count(-1):\n",
    "                topic_model.update_topics(lemma_dc, topics=new_topics, vectorizer_model=vectorizer_model,\n",
    "                                          ctfidf_model=ctfidf_model, representation_model=representation_model)\n",
    "        else:\n",
    "            print(\"No outliers found â€” skipping reduction.\")\n",
    "\n",
    "        topics_dict = topic_model.get_topics()\n",
    "        topics_labels = topics\n",
    "        topics_list = extract_topic_words(topics_dict, topk=10)\n",
    "\n",
    "        coh = topic_coherence(topics_list, lemma_dc, topk=10)\n",
    "        div = topic_diversity(topics_dict, topk=10)\n",
    "        sil = topic_silhouette(embedding, topics_labels)\n",
    "\n",
    "        print(f\"Coherence (c_v): {coh:.4f}\")\n",
    "        print(f\"Diversity: {div:.4f}\")\n",
    "        print(f\"Silhouette (cos): {sil:.4f}\")\n",
    "\n",
    "        return topic_model, topics, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d0b83a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 1/30: max_df=0.900, n_neighbors=5, min_samples=25\n",
      "Testing combination 2/30: max_df=0.950, n_neighbors=20, min_samples=15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 08:54:46,420 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 3/30: max_df=0.850, n_neighbors=20, min_samples=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 08:55:28,771 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 4/30: max_df=0.450, n_neighbors=15, min_samples=10\n",
      "Testing combination 5/30: max_df=0.700, n_neighbors=20, min_samples=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 08:56:50,645 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 6/30: max_df=0.400, n_neighbors=15, min_samples=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 08:57:34,630 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 7/30: max_df=0.450, n_neighbors=25, min_samples=35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 08:58:21,905 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 8/30: max_df=0.400, n_neighbors=10, min_samples=15\n",
      "Testing combination 9/30: max_df=0.700, n_neighbors=20, min_samples=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 08:59:47,220 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 10/30: max_df=0.650, n_neighbors=5, min_samples=30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:00:30,034 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 11/30: max_df=0.750, n_neighbors=35, min_samples=15\n",
      "Testing combination 12/30: max_df=0.900, n_neighbors=35, min_samples=20\n",
      "Testing combination 13/30: max_df=0.750, n_neighbors=10, min_samples=40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:02:35,263 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 14/30: max_df=0.400, n_neighbors=35, min_samples=20\n",
      "Testing combination 15/30: max_df=0.800, n_neighbors=20, min_samples=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:03:53,408 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 16/30: max_df=0.900, n_neighbors=15, min_samples=25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:04:37,283 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 17/30: max_df=0.500, n_neighbors=15, min_samples=30\n",
      "Testing combination 18/30: max_df=0.800, n_neighbors=20, min_samples=35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:06:10,613 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 19/30: max_df=0.400, n_neighbors=30, min_samples=35\n",
      "Testing combination 20/30: max_df=0.650, n_neighbors=15, min_samples=5\n",
      "Testing combination 21/30: max_df=0.850, n_neighbors=15, min_samples=30\n",
      "Testing combination 22/30: max_df=0.450, n_neighbors=15, min_samples=5\n",
      "Testing combination 23/30: max_df=0.400, n_neighbors=5, min_samples=40\n",
      "Testing combination 24/30: max_df=0.750, n_neighbors=20, min_samples=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:10:20,437 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 25/30: max_df=0.400, n_neighbors=30, min_samples=20\n",
      "Testing combination 26/30: max_df=0.800, n_neighbors=25, min_samples=40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:11:38,082 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 27/30: max_df=0.700, n_neighbors=35, min_samples=15\n",
      "Testing combination 28/30: max_df=0.400, n_neighbors=30, min_samples=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:12:55,777 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 29/30: max_df=0.650, n_neighbors=35, min_samples=35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 09:13:37,141 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 30/30: max_df=0.700, n_neighbors=10, min_samples=30\n",
      "\n",
      "Top 5 Parameter Combinations:\n",
      "\n",
      "Rank 1:\n",
      "max_df: 0.750, n_neighbors: 10, min_samples: 40\n",
      "Coherence: 0.4437, Diversity: 0.7714, Silhouette: 0.1712\n",
      "Combined Score: 0.4621\n",
      "\n",
      "Rank 2:\n",
      "max_df: 0.450, n_neighbors: 25, min_samples: 35\n",
      "Coherence: 0.4709, Diversity: 0.7625, Silhouette: 0.0918\n",
      "Combined Score: 0.4417\n",
      "\n",
      "Rank 3:\n",
      "max_df: 0.800, n_neighbors: 20, min_samples: 20\n",
      "Coherence: 0.4485, Diversity: 0.7750, Silhouette: 0.0988\n",
      "Combined Score: 0.4408\n",
      "\n",
      "Rank 4:\n",
      "max_df: 0.400, n_neighbors: 30, min_samples: 10\n",
      "Coherence: 0.4620, Diversity: 0.7375, Silhouette: 0.1203\n",
      "Combined Score: 0.4399\n",
      "\n",
      "Rank 5:\n",
      "max_df: 0.700, n_neighbors: 10, min_samples: 30\n",
      "Coherence: 0.3855, Diversity: 0.8071, Silhouette: 0.0345\n",
      "Combined Score: 0.4090\n",
      "\n",
      "Grid search results saved to: e:\\Studying in Adelaide\\2_Trimester-2\\project_A_ML-Mental Health (MDS)\\LLM_NetworkMoel_MentalHealth_coding\\topic_modeling_results_suicidal_thoughts_and_self-harm.csv\n"
     ]
    }
   ],
   "source": [
    "platform = 'beyondblue'\n",
    "use_pretrained_embed = True\n",
    "# # Example usage for Anxiety\n",
    "# stopwords = ['feel', 'week', 'tng', 'need', 'say', 'hard', 'good', 'sometng',\n",
    "#              'work', 'help', 'talk', 'ask', 'end', 'start', 'people', 'month',\n",
    "#              'thought', 'way', 'anytng', 'day', 'make', 'year', 'everytng', 'fly',\n",
    "#              'experience', 'health', 'drive', 'feeling', 'kind', 'manage', 'mental',\n",
    "#              'understand', 'mind', 'new', 'hear', 'right', 'lm', 'tell', 'hello', 'body',\n",
    "#              'meet', 'past', 'self', 'follow', 'try', 'walk', 'wiht', 'use', 'act', 'wle', 'welcome']\n",
    "# label = \"Anxiety\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Depression\n",
    "# stopwords=['sleep', 'kind', 'good', 'love', 'need', 'sometng', 'feel', 'talk', 'home',\n",
    "#            'tng', 'everytng', 'anytng', 'notng', 'self','people', 'work', 'way',\n",
    "#             'tell',  'experience', 'wle', 'health','wle','make','help', 'say',\n",
    "#             'day','right', 'hear', 'lead','thought', 've','end','week','use',\n",
    "#             'word','ask','come', 'sure','mean','lot' ]\n",
    "# label = \"Depression\"\n",
    "\n",
    "\n",
    "\n",
    "# PTSD and trauma\n",
    "# stopwords=['tng', 'love', 'good', 'need', 'way',  'feel', 'talk', 'sometng','help',\n",
    "#            'people', 'work', 'tell',  'hope',  'sorry','hard', 'right','say','end',\n",
    "#            'week','everytng', 'anytng','gh', 'make','mh','feeling', 'thought', 'situation',\n",
    "#            'hear', 'long','past', 'like','person', 'mind','ask', 'womb', 'welcome',\n",
    "#            'use','sort', 'result', 'write','day','mean','friendsp','wle', 'self' ]\n",
    "# label = \"PTSD and trauma\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Suicidal thoughts and self-harm\n",
    "stopwords=['tng', 'need', 'good', 'feel', 'talk', 'friend', 'love', 'hear', 'family', 'tell',\n",
    "           'way', 'make', 'sometng', 'self', 'say',  'end','support', 'post', 'understand',\n",
    "           'live', 'service', 'experience', 'leave', 'sound', 'welcome', 'long','wle', 'let',\n",
    "            'mental health', 'anytng', 'everytng','use', 'wonder','thought','anymore','work',\n",
    "            'hard', 'hope', 'day', 'feeling', 'mental', 'right', 'person','sope', 'reacng',\n",
    "            'sorry hear','start', 'write','sh thought', 'skill', 'look', 'mean','people', \n",
    "            'try', 'care', 'health',  'kind',  'moment','year', 'week', 'ask','mind','lucys',\n",
    "            'variable','kalice', 'nice story','okpitch', 'sit', 'dear okpitch','notng', 'calli',\n",
    "            'tiah', 'stay', 'jessksch','tony', 'old','mum', 'beekay','tnke','place', ]\n",
    "label = \"Suicidal thoughts and self-harm\"\n",
    "\n",
    "\n",
    "\n",
    "original_cols = [\"Post Title\", \"Post Content\", \"Comments\"]\n",
    "min_df = 1\n",
    "max_df = 0.95\n",
    "n_neighbors = 15\n",
    "min_cluster_size = 30\n",
    "min_samples = 10\n",
    "nr_topics = 'auto'\n",
    "\n",
    "\n",
    "# Run grid search\n",
    "top_5_results = topic_modeling(\n",
    "    platform=platform,\n",
    "    label=label,\n",
    "    original_cols=original_cols,\n",
    "    stopwords=stopwords,\n",
    "    min_df=min_df,\n",
    "    max_df=max_df,\n",
    "    n_neighbors=n_neighbors,\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    min_samples=min_samples,\n",
    "    nr_topics=nr_topics,\n",
    "    use_pretrained_embed=use_pretrained_embed,\n",
    "    grid_search=True,\n",
    "    n_samples=30\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDS1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
