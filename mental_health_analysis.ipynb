{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620d51aa",
   "metadata": {},
   "source": [
    "# Mental Health Analysis ProjectA \n",
    "\n",
    "The purpose of this project is ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa92f41",
   "metadata": {},
   "source": [
    "# Data Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1710d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_symptom_categories = [\n",
    "    \"Depression\",\n",
    "    \"Anxiety\",\n",
    "    \"Stress/Burnout\",\n",
    "    \"Loneliness\",\n",
    "    \"Low Self-Esteem\",\n",
    "    \"Suicidal Ideation\",\n",
    "    \"Trauma/PTSD\",\n",
    "    \"Anger/Irritability\",\n",
    "    \"Obsessive Thoughts\",\n",
    "    \"Addiction/Substance Use\"\n",
    "]\n",
    "\n",
    "suicide_and_selfharm_labels = [\n",
    "    \"Suicidal Ideation\",\n",
    "    \"Suicide Attempt\",\n",
    "    \"Self-Harm (Non-Suicidal)\",\n",
    "    \"Hopelessness / Despair\",\n",
    "    \"Crisis / Urgent Help Request\",\n",
    "    \"Grief After Suicide\",\n",
    "    \"Recovery / Coping with Suicidal Thoughts\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bbce6f",
   "metadata": {},
   "source": [
    "Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping data from reddit api\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import praw\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RedditScraper:\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.auth = requests.auth.HTTPBasicAuth(os.getenv('CLIENT_ID'), os.getenv('CLIENT_SECRET'))\n",
    "        self.data = {'grant_type': 'password',\n",
    "                     'username': os.getenv('USERNAME'),\n",
    "                     'password': os.getenv('PASSWORD')}\n",
    "        self.headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "        self.res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                                auth=self.auth, data=self.data, headers=self.headers)\n",
    "        \n",
    "        self.headers[\"Authorization\"] = f'bearer {self.res.json()[\"access_token\"]}'\n",
    "\n",
    "    def get_posts_byrequests(self, subreddit, limit=1000):\n",
    "        url = f'https://oauth.reddit.com/r/{subreddit}/hot'\n",
    "        params = {'limit': limit}\n",
    "        response = requests.get(url, headers=self.headers, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        else:\n",
    "            raise Exception(f\"Error fetching posts: {response.status_code} - {response.text}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_posts_byprawn(self, subreddits, limit=1000, mental=\"mental_\"):\n",
    "\n",
    "        reddit = praw.Reddit(client_id=os.getenv('CLIENT_ID'),\n",
    "                             client_secret=os.getenv('CLIENT_SECRET'),\n",
    "                             user_agent='windows:mentalhealth.scraper:v1.0 (by u/IceWorth5480)',\n",
    "                             username=os.getenv('USERNAME'),\n",
    "                             password=os.getenv('PASSWORD'))\n",
    "        \n",
    "        all_posts = []\n",
    "        sort_types = ['hot', 'top', 'new']\n",
    "\n",
    "\n",
    "        for subreddit in subreddits:\n",
    "            for sort in sort_types:\n",
    "                if sort == 'hot':\n",
    "                    posts = reddit.subreddit(subreddit).hot(limit=limit)\n",
    "                elif sort == 'top':\n",
    "                    posts = reddit.subreddit(subreddit).top(limit=limit)\n",
    "                elif sort == 'new':\n",
    "                    posts = reddit.subreddit(subreddit).new(limit=limit)\n",
    "\n",
    "                for post in posts:\n",
    "                    if post is None:\n",
    "                        continue\n",
    "                    all_posts.append({\n",
    "                        'id': post.id,\n",
    "                        'subreddit': subreddit,\n",
    "                        'sort': sort,  # Add sort type for tracking\n",
    "                        'title': post.title,\n",
    "                        'selftext': post.selftext,\n",
    "                        'created_utc': post.created_utc,\n",
    "                        'score': post.score,\n",
    "                        'num_comments': post.num_comments,\n",
    "                        'author': str(post.author),\n",
    "                        'url': post.url,\n",
    "                        'over_18': post.over_18,\n",
    "                        'flair': post.link_flair_text\n",
    "                    })\n",
    "\n",
    "\n",
    "        # delete dupicates by id\n",
    "        all_posts = [dict(t) for t in {tuple(d.items()) for d in all_posts}]\n",
    " \n",
    "\n",
    "\n",
    "        df = pd.DataFrame(all_posts)\n",
    "        df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "        df = df.sort_values(by='created_utc', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        # check if f'./data/reddit_data/{mental}reddit_posts.csv' exists, if so merging with df and # delete dupicates by id\n",
    "        if os.path.exists(f'./data/reddit_data/{mental}reddit_posts.csv'):\n",
    "            existing_df = pd.read_csv(f'./data/reddit_data/{mental}reddit_posts.csv')\n",
    "            df = pd.concat([existing_df, df]).drop_duplicates(subset='id').reset_index(drop=True)\n",
    "       \n",
    "        # save as csv \n",
    "        df.to_csv(f'./data/reddit_data/{mental}reddit_posts.csv', index=False)\n",
    "\n",
    "\n",
    "        \n",
    "    def get_posts_byPushshift(self, subreddit, limit=1000):\n",
    "        url = f'https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit}&sort=desc&limit={limit}'\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        print(response.status_code)\n",
    "        print(response.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mental_subreddits = ['mentalhealth', 'depression', 'anxiety', 'therapy', 'selfhelp', 'bpd', 'ptsd', 'socialanxiety', 'counseling']\n",
    "normal_subreddits = ['popular','all','AskReddit','interestingasfuck']\n",
    "test = RedditScraper()\n",
    "\n",
    "test.get_posts_byprawn(mental_subreddits, limit=10,mental=\"mental_\")\n",
    "test.get_posts_byprawn(normal_subreddits, limit=10, mental=\"normal_\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5613c",
   "metadata": {},
   "source": [
    "Beyond Blue forums "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TO DO\n",
    "\n",
    "Extract following information from the reddit webpage \n",
    "\n",
    "\n",
    "Post ID: A unique identifier for each post.\n",
    "Post Content: The text of the post.\n",
    "Post Author: The author of the post.\n",
    "Post Date: The date the post was made.\n",
    "Post Category: Category or forum where the post was made.\n",
    "Number of Comments: The total number of comments on the post.\n",
    "\n",
    "From Comment \n",
    "\n",
    "Post ID: Link back to the original post.\n",
    "Comment ID: A unique identifier for each comment.\n",
    "Comment Content: Text of the comment.\n",
    "Comment Author: Author of the comment.\n",
    "Comment Date: Date the comment was posted. (the order of the comments is really important)\n",
    "other meta data if available\n",
    "'''\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Setup Chrome WebDriver\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf93aa",
   "metadata": {},
   "source": [
    "Climate data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53210b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78036975",
   "metadata": {},
   "source": [
    "# Nature Language Process\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
