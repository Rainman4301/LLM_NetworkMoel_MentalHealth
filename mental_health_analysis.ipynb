{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620d51aa",
   "metadata": {},
   "source": [
    "# Mental Health Analysis ProjectA \n",
    "\n",
    "The purpose of this project is ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa92f41",
   "metadata": {},
   "source": [
    "# Data Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bbce6f",
   "metadata": {},
   "source": [
    "Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d39e0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping data from reddit api\n",
    "import requests\n",
    "import pandas as pd\n",
    "# import praw\n",
    "import emoji\n",
    "import emot\n",
    "import asyncpraw\n",
    "# import asyncio\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RedditScraper:\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.auth = requests.auth.HTTPBasicAuth(os.getenv('CLIENT_ID'), os.getenv('CLIENT_SECRET'))\n",
    "        self.data = {'grant_type': 'password',\n",
    "                     'username': os.getenv('USERNAME'),\n",
    "                     'password': os.getenv('PASSWORD')}\n",
    "        self.headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "        self.res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                                auth=self.auth, data=self.data, headers=self.headers)\n",
    "        \n",
    "        self.headers[\"Authorization\"] = f'bearer {self.res.json()[\"access_token\"]}'\n",
    "\n",
    "\n",
    "\n",
    "        self.client_id = os.getenv('CLIENT_ID')\n",
    "        self.client_secret = os.getenv('CLIENT_SECRET')\n",
    "        self.username = os.getenv('USERNAME')\n",
    "        self.password = os.getenv('PASSWORD')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_posts_byrequests(self, subreddit, limit=1000):\n",
    "        url = f'https://oauth.reddit.com/r/{subreddit}/hot'\n",
    "        params = {'limit': limit}\n",
    "        response = requests.get(url, headers=self.headers, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        else:\n",
    "            raise Exception(f\"Error fetching posts: {response.status_code} - {response.text}\")\n",
    "        \n",
    "\n",
    "\n",
    "    def convert_emojis_emoticons(self, text):\n",
    "        e = emot.core.emot()\n",
    "\n",
    "        # Extract emoticons\n",
    "        emoticon_results = e.emoticons(text)\n",
    "        for original, meaning in zip(emoticon_results['value'], emoticon_results['mean']):\n",
    "            text = text.replace(original, f\" {meaning} \")\n",
    "\n",
    "        # Extract emojis\n",
    "        # emoji_results = e.emoji(text)\n",
    "        # for original, meaning in zip(emoji_results['value'], emoji_results['mean']):\n",
    "        #     text = text.replace(original, f\" {meaning} \")\n",
    "\n",
    "        text = emoji.demojize(text)\n",
    "\n",
    "\n",
    "\n",
    "        return text.strip().lower()\n",
    "        \n",
    "\n",
    "\n",
    "    async def get_posts_byprawn(self, subreddits, limit=1000, mental=\"mental_\"):\n",
    "\n",
    "        reddit = asyncpraw.Reddit(client_id=self.client_id,\n",
    "                             client_secret=self.client_secret,\n",
    "                             user_agent='windows:mentalhealth.scraper:v1.0 (by u/IceWorth5480)',\n",
    "                             username=self.username,\n",
    "                             password=self.password)\n",
    "        \n",
    "        all_posts = []\n",
    "        # , 'top', 'new'\n",
    "        sort_types = ['hot']\n",
    "\n",
    "\n",
    "        for subreddit_name in tqdm(subreddits, desc=\"Subreddits Progress\"):\n",
    "            subreddit = await reddit.subreddit(subreddit_name)\n",
    "            for sort in sort_types:\n",
    "                if sort == 'hot':\n",
    "                    posts = subreddit.hot(limit=limit)\n",
    "                elif sort == 'top':\n",
    "                    posts = subreddit.top(limit=limit)\n",
    "                elif sort == 'new':\n",
    "                    posts = subreddit.new(limit=limit)\n",
    "\n",
    "                async for post in posts:\n",
    "                    if post is None:\n",
    "                        continue\n",
    "\n",
    "                    # Load top-level comments (non-blocking)\n",
    "                    await post.load()\n",
    "                    await post.comments.replace_more(limit=0)\n",
    "                    top_comments_raw = [comment.body for comment in post.comments[:5]]  # Get top 5 comments\n",
    "                    top_comments = [self.convert_emojis_emoticons(c) for c in top_comments_raw]\n",
    "\n",
    "                    all_posts.append({\n",
    "                        'id': post.id,\n",
    "                        'subreddit': subreddit_name,\n",
    "                        'sort': sort,\n",
    "                        'title': post.title,\n",
    "                        'selftext': self.convert_emojis_emoticons(post.selftext),\n",
    "                        'created_utc': post.created_utc,\n",
    "                        'score': post.score,\n",
    "                        'num_comments': post.num_comments,\n",
    "                        'author': str(post.author),\n",
    "                        'post_url': post.url,\n",
    "                        'over_18': post.over_18,\n",
    "                        'flair': post.link_flair_text,\n",
    "                        'top_comments': top_comments\n",
    "                    })\n",
    "\n",
    "        await reddit.close()\n",
    "\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(all_posts)\n",
    "        # Drop duplicates by post ID\n",
    "        df = df.drop_duplicates(subset='id').reset_index(drop=True)\n",
    "        df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "        df = df.sort_values(by='created_utc', ascending=False).reset_index(drop=True)\n",
    "        # Convert list of comments to string for CSV storage\n",
    "        df['top_comments'] = df['top_comments'].apply(lambda x: ' | '.join(x) if isinstance(x, list) else '')\n",
    "\n",
    "\n",
    "\n",
    "        # check if f'./data/reddit_data/{mental}reddit_posts.csv' exists, if so merging with df and # delete dupicates by id\n",
    "        if os.path.exists(f'./data/reddit_data/{mental}reddit_posts.csv'):\n",
    "            existing_df = pd.read_csv(f'./data/reddit_data/{mental}reddit_posts.csv')\n",
    "            df = pd.concat([existing_df, df]).drop_duplicates(subset='id').reset_index(drop=True)\n",
    "       \n",
    "        # save as csv \n",
    "        df.to_csv(f'./data/reddit_data/{mental}reddit_posts.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d057b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subreddits Progress: 100%|██████████| 4/4 [1:20:33<00:00, 1208.28s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "mental_subreddits = ['mentalhealth', 'depression', 'anxiety', 'therapy', 'selfhelp', 'bpd', 'ptsd', 'socialanxiety', 'counseling']\n",
    "normal_subreddits = ['popular','all','AskReddit','interestingasfuck']\n",
    "# australian_regions = ['melbourne','sydney','adelaide','perth','brisbane','canberra']\n",
    "\n",
    "\n",
    "scraper  = RedditScraper()\n",
    "\n",
    "# await scraper.get_posts_byprawn(mental_subreddits, limit=1000, mental=\"mental_\")\n",
    "await scraper.get_posts_byprawn(normal_subreddits, limit=1000, mental=\"normal_\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7646, 13)\n",
      "(180, 13)\n"
     ]
    }
   ],
   "source": [
    "# read the csv file \n",
    "\n",
    "df_mental = pd.read_csv('./data/reddit_data/mental_reddit_posts.csv')\n",
    "print(df_mental.shape)\n",
    "\n",
    "df_normal = pd.read_csv('./data/reddit_data/normal_reddit_posts.csv')\n",
    "print(df_normal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5613c",
   "metadata": {},
   "source": [
    "Beyond Blue forums "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175d8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TO DO\n",
    "\n",
    "Extract following information from the reddit webpage \n",
    "\n",
    "\n",
    "Post ID: A unique identifier for each post.\n",
    "Post Content: The text of the post.\n",
    "Post Author: The author of the post.\n",
    "Post Date: The date the post was made.\n",
    "Post Category: Category or forum where the post was made.\n",
    "Number of Comments: The total number of comments on the post.\n",
    "\n",
    "From Comment \n",
    "\n",
    "Post ID: Link back to the original post.\n",
    "Comment ID: A unique identifier for each comment.\n",
    "Comment Content: Text of the comment.\n",
    "Comment Author: Author of the comment.\n",
    "Comment Date: Date the comment was posted. (the order of the comments is really important)\n",
    "other meta data if available\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import os\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "import emot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_post_date(raw_date):\n",
    "\n",
    "    weekdays = [day.lower() for day in list(calendar.day_name) ]\n",
    "    today = datetime.now()\n",
    "\n",
    "    raw_date = raw_date.strip().lower()  \n",
    "\n",
    "    # Case 1: Day of the week (e.g., \"Monday\")\n",
    "    if raw_date in weekdays:\n",
    "        # Get weekday index: Monday = 0, Sunday = 6\n",
    "        post_weekday_index = weekdays.index(raw_date)\n",
    "        today_weekday_index = today.weekday()\n",
    "\n",
    "        # Calculate difference in days\n",
    "        delta_days = (today_weekday_index - post_weekday_index) % 7\n",
    "        # Get actual date\n",
    "        post_date = today - timedelta(days=delta_days)\n",
    "        return post_date.strftime('%Y-%m-%d')  # Format as YYYY-MM-DD\n",
    "    \n",
    "    # Case 2: \"a week ago\", \"2 weeks ago\", etc.\n",
    "    elif \"week\" in raw_date:\n",
    "        match = re.search(r'(\\d+)', raw_date)\n",
    "        weeks = int(match.group(1)) if match else 1\n",
    "        post_date = today - timedelta(weeks=weeks)\n",
    "        return post_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Case 3: \"a month ago\", \"2 months ago\", etc.\n",
    "    elif \"month\" in raw_date:\n",
    "        match = re.search(r'(\\d+)', raw_date)\n",
    "        months = int(match.group(1)) if match else 1\n",
    "        # Approximate a month as 30 days\n",
    "        post_date = today - timedelta(days=30 * months)\n",
    "        return post_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Case 4: Exact date format like \"11-05-2025\"\n",
    "    else:\n",
    "        # Try parsing date in the format like \"25-09-2020\"\n",
    "        try:\n",
    "            post_date = datetime.strptime(raw_date, '%d-%m-%Y')\n",
    "            return post_date.strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            return 'Unknown date'  # If format is unexpected\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_emojis_emoticons(text):\n",
    "    e = emot.core.emot()\n",
    "\n",
    "    # Extract emoticons\n",
    "    emoticon_results = e.emoticons(text)\n",
    "\n",
    "    for original, meaning in zip(emoticon_results['value'], emoticon_results['mean']):\n",
    "        text = text.replace(original, f\" {meaning} \")\n",
    "\n",
    "    # # Extract emojis\n",
    "    # emoji_results = e.emoji(text)\n",
    "    # for original, meaning in zip(emoji_results['value'], emoji_results['mean']):\n",
    "    #     text = text.replace(original, f\" {meaning} \")\n",
    "\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def comment_scrapping (url, comment_pages = 1):\n",
    "\n",
    "    \n",
    "    # Setup Chrome WebDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    comment_driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    list_comments = []\n",
    "\n",
    "    for page in range(1, comment_pages + 1):\n",
    "\n",
    "        comment_driver.get(url)\n",
    "        time.sleep(0.005)\n",
    "        soup = BeautifulSoup(comment_driver.page_source, 'html.parser')\n",
    "        comments_section = soup.find('div', class_='linear-message-list message-list')\n",
    "        every_comments = comments_section.find_all('div')\n",
    "\n",
    "\n",
    "        for comment in every_comments:\n",
    "\n",
    "            main_section = comment.find('div', class_='lia-quilt-row lia-quilt-row-message-main') if comment.find('div', class_='lia-quilt-row lia-quilt-row-message-main') else None\n",
    "            if not main_section:\n",
    "                continue\n",
    "            # scrapping all the text \n",
    "            comment_text = main_section.get_text(separator=' ', strip=True) if main_section else \"\"\n",
    "            comment_text = convert_emojis_emoticons(comment_text)\n",
    "            list_comments.append(comment_text)\n",
    "\n",
    "            # only scrapping the first 3 comments\n",
    "            if len(list_comments) >= 3:\n",
    "                break\n",
    "\n",
    "        # Find next page link\n",
    "        next_page = soup.find('li', class_='lia-paging-page-next')\n",
    "        if next_page and next_page.find('a'):\n",
    "            next_page_link = next_page.find('a')['href']\n",
    "            url = next_page_link\n",
    "        else:\n",
    "            print(\"No more pages to scrape.\")\n",
    "            break\n",
    "        \n",
    "    # close the driver\n",
    "    comment_driver.quit()\n",
    "\n",
    "    # change the list of comments to a string\n",
    "    list_comments = ' | '.join(list_comments) if list_comments else \"\"\n",
    "    \n",
    "\n",
    "    return list_comments\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def beyondblue_scrapping(tag,address,pages=2):\n",
    "\n",
    "\n",
    "    # Setup Chrome WebDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    url = address\n",
    "\n",
    "    whole_data = []\n",
    "\n",
    "\n",
    "\n",
    "    for page in tqdm(range(1, pages + 1), desc=\"Scraping pages\"):\n",
    "\n",
    "    \n",
    "        driver.get(url)\n",
    "        # time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "\n",
    "        # get the class \"custom-message-list all-discussions\"\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "\n",
    "        discussions = soup.find('div', class_='custom-message-list all-discussions').find_all(('article'))\n",
    "        \n",
    "\n",
    "        for post in discussions:\n",
    "\n",
    "            # Extracting post id\n",
    "            post_link = post.find('h3').find_all('a')[1].get('href')\n",
    "            post_id = post_link.split('/')[-1]  \n",
    "\n",
    "\n",
    "            full_post_link = f\"https://forums.beyondblue.org.au{post_link}\"\n",
    "            # extract comments from post_link\n",
    "            comments = comment_scrapping(full_post_link, comment_pages=1)\n",
    "            \n",
    "\n",
    "            # Extracting post title\n",
    "            title_tag = post.find_all('h3')[0].find_all('a')[1]\n",
    "            post_title = convert_emojis_emoticons(title_tag.text.strip()) if title_tag else \"\"\n",
    "\n",
    "            # Extracting post content\n",
    "            post_content = convert_emojis_emoticons(post.find('p', class_ = 'body-text').text.strip()) if post.find('p', class_ = 'body-text') else \"\"\n",
    "            \n",
    "\n",
    "            side_info = post.find('aside')\n",
    "            side_info1 = side_info.find('div', class_='custom-tile-author-info') if side_info else None\n",
    "            # Extracting post author\n",
    "            post_author = side_info1.find('a').find('span').text.strip() if side_info and side_info.find('span') else \"\"\n",
    "\n",
    "            author_link = post.find('aside').find('div', class_='custom-tile-author-info').find('a').get('href')\n",
    "            # Extracting user id from author link\n",
    "            user_id = author_link.split('user-id/')[-1]\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            side_info2 = side_info.find('div', class_='custom-tile-category-content')\n",
    "            # Extracting post tag\n",
    "            post_tag = side_info2.find('a').text.strip() if side_info2 and side_info2.find('a') else \"\"\n",
    "            \n",
    "            \n",
    "            raw_date = side_info2.find('time').text.strip() if side_info2 and side_info2.find('time') else \"\"\n",
    "            # Extracting post date\n",
    "            post_date = parse_post_date(raw_date)\n",
    "\n",
    "\n",
    "\n",
    "            side_info3 = side_info.find('div', class_='custom-tile-unread-replies')\n",
    "            unread = side_info3.find('span').text.strip() if side_info3 and side_info3.find('span') else \"\"\n",
    "            # Extracting number of unread replies\n",
    "            match = re.search(r'\\d+', unread)\n",
    "            post_unread = int(match.group()) if match else 0\n",
    "\n",
    "            # Extracting number of comments\n",
    "            number_comments = post.find('li', class_ = 'custom-tile-replies').find('b').text.strip() if post.find('li', class_ = 'custom-tile-replies') else \"\"\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            post_data = {\n",
    "                \"Post ID\": post_id,\n",
    "                \"Post Title\": post_title,\n",
    "                \"Post Content\": post_content,\n",
    "                \"Post Author\": post_author,\n",
    "                \"User ID\": user_id,\n",
    "                \"Post Date\": post_date,\n",
    "                \"Post Category\": post_tag,\n",
    "                \"Number of Comments\": number_comments,\n",
    "                \"Comments\": comments\n",
    "            }\n",
    "            whole_data.append(post_data)\n",
    "\n",
    "\n",
    "        # Print the number of posts scraped on the current page\n",
    "        # print(f\"Tag: {tag} Page {page}: Scraped {len(discussions)} posts.\")\n",
    "\n",
    "        # Find next page link\n",
    "        next_page = soup.find('li', class_='lia-paging-page-next')\n",
    "        if next_page and next_page.find('a'):\n",
    "            url = next_page.find('a')['href']\n",
    "        else:\n",
    "            print(\"No more pages to scrape.\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if page % 10 == 0:\n",
    "            # save the data to a csv file\n",
    "            temp_df = pd.DataFrame(whole_data)\n",
    "            temp_df = temp_df.drop_duplicates(subset='Post ID').reset_index(drop=True)\n",
    "            temp_df['Post Date'] = pd.to_datetime(temp_df['Post Date'], errors='coerce')\n",
    "            temp_df = temp_df.sort_values(by='Post Date', ascending=False).reset_index(drop=True)\n",
    "            # check if the file already exists\n",
    "            if not os.path.exists(f'./data/beyondblue_data/{tag}_beyondblue_posts.csv'):\n",
    "                temp_df.to_csv(f'./data/beyondblue_data/{tag}_beyondblue_posts.csv', index=False)\n",
    "            else:\n",
    "                existing_df = pd.read_csv(f'./data/beyondblue_data/{tag}_beyondblue_posts.csv')\n",
    "                temp_df = pd.concat([existing_df, temp_df]).drop_duplicates(subset='Post ID').reset_index(drop=True)\n",
    "                temp_df.to_csv(f'./data/beyondblue_data/{tag}_beyondblue_posts.csv', index=False)\n",
    "\n",
    "            print(f'The data for tag {tag} has been scraped to page {page}')\n",
    "\n",
    "            # if the oldest post is older than 2020-01-01, stop the scraping\n",
    "            # if not temp_df.empty and temp_df['Post Date'].min() < pd.to_datetime('2020-01-01'):\n",
    "            #     print(f\"The oldest post is older than 2020-01-01, stopping the scraping for tag {tag}.\")\n",
    "            #     break\n",
    "\n",
    "\n",
    "    #close the driver\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Convert to DataFrame \n",
    "    df = pd.DataFrame(whole_data)\n",
    "    # delete duplicates by Post ID\n",
    "    df = df.drop_duplicates(subset='Post ID').reset_index(drop=True)\n",
    "    # Convert 'Post Date' to datetime format\n",
    "    df['Post Date'] = pd.to_datetime(df['Post Date'], errors='coerce')\n",
    "    # Sort by 'Post Date' in descending order\n",
    "    df = df.sort_values(by='Post Date', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # check if the file already exists\n",
    "    if os.path.exists(f'./data/beyondblue_data/{tag}_beyondblue_posts.csv'):\n",
    "        existing_df = pd.read_csv(f'./data/beyondblue_data/{tag}_beyondblue_posts.csv')\n",
    "        df = pd.concat([existing_df, df]).drop_duplicates(subset='Post ID').reset_index(drop=True)\n",
    "\n",
    "    df.to_csv(f'./data/beyondblue_data/{tag}_beyondblue_posts.csv', index=False)\n",
    "        \n",
    "        \n",
    "    print(f\"Data saved to ./data/beyondblue_data/{tag}_beyondblue_posts.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df711d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:   7%|▋         | 10/150 [13:56<3:14:03, 83.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for tag Relationship_family_issues has been scraped to page 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  13%|█▎        | 20/150 [27:34<2:56:11, 81.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for tag Relationship_family_issues has been scraped to page 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  20%|██        | 30/150 [41:03<2:41:22, 80.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for tag Relationship_family_issues has been scraped to page 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  27%|██▋       | 40/150 [54:35<2:28:14, 80.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for tag Relationship_family_issues has been scraped to page 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  33%|███▎      | 50/150 [1:08:00<2:15:13, 81.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for tag Relationship_family_issues has been scraped to page 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  40%|████      | 60/150 [1:21:43<2:05:01, 83.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for tag Relationship_family_issues has been scraped to page 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  47%|████▋     | 70/150 [1:35:30<1:49:06, 81.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for tag Relationship_family_issues has been scraped to page 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  53%|█████▎    | 80/150 [1:49:00<1:34:56, 81.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for tag Relationship_family_issues has been scraped to page 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  60%|██████    | 90/150 [2:02:33<1:21:09, 81.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for tag Relationship_family_issues has been scraped to page 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  67%|██████▋   | 100/150 [2:16:09<1:08:32, 82.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for tag Relationship_family_issues has been scraped to page 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  73%|███████▎  | 110/150 [2:29:57<55:00, 82.52s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for tag Relationship_family_issues has been scraped to page 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  80%|████████  | 120/150 [2:43:46<41:34, 83.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for tag Relationship_family_issues has been scraped to page 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  87%|████████▋ | 130/150 [2:57:29<27:30, 82.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for tag Relationship_family_issues has been scraped to page 130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  93%|█████████▎| 140/150 [3:11:10<13:45, 82.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for tag Relationship_family_issues has been scraped to page 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 150/150 [3:24:55<00:00, 81.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data for tag Relationship_family_issues has been scraped to page 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/beyondblue_data/Relationship_family_issues_beyondblue_posts.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mental_health_urls = {\n",
    "    \"Anxiety\": \"https://forums.beyondblue.org.au/t5/anxiety/bd-p/c1-sc2-b1?&sort=recent\",\n",
    "    \"Depression\": \"https://forums.beyondblue.org.au/t5/depression/bd-p/c1-sc2-b2?&sort=recent\",\n",
    "    \"PTSD\": \"https://forums.beyondblue.org.au/t5/ptsd-and-trauma/bd-p/c1-sc2-b3?&sort=recent\",\n",
    "    \"Suicide_selfharm\": \"https://forums.beyondblue.org.au/t5/suicidal-thoughts-and-self-harm/bd-p/c1-sc2-b4?&sort=recent\",\n",
    "    \"Staying_well\": \"https://forums.beyondblue.org.au/t5/staying-well/bd-p/c1-sc3-b1?&sort=recent\",\n",
    "    \"Treament\": \"https://forums.beyondblue.org.au/t5/treatments-health-professionals/bd-p/c1-sc3-b2?&sort=recent\",\n",
    "    \"Relationship_family_issues\":\"https://forums.beyondblue.org.au/t5/relationship-and-family-issues/bd-p/c1-sc3-b3?&sort=recent\",\n",
    "    \"Youth\":\"https://forums.beyondblue.org.au/t5/young-people/bd-p/c1-sc4-b1?&sort=recent\",\n",
    "    \"Sex_identity\":\"https://forums.beyondblue.org.au/t5/sexuality-and-gender-identity/bd-p/c1-sc4-b2?&sort=recent\",\n",
    "    \"Multiculture\":\"https://forums.beyondblue.org.au/t5/multicultural-experiences/bd-p/c1-sc4-b3?&sort=recent\",\n",
    "    \"Grief_loss\":\"https://forums.beyondblue.org.au/t5/grief-and-loss/bd-p/c1-sc4-b4?&sort=recent\"\n",
    "}\n",
    "\n",
    "each_tag_number_data = 1500\n",
    "pages = each_tag_number_data // 10  # Assuming each page has 10 posts\n",
    "\n",
    "\n",
    "for tag, address in mental_health_urls.items():\n",
    "    try:\n",
    "        beyondblue_scrapping(tag, address, pages = pages)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {tag}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf93aa",
   "metadata": {},
   "source": [
    "Climate data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b60e3cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing Sydney - Rainfall: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=137.0.7151.104)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0x943b03+62899]\n",
      "\tGetHandleVerifier [0x0x943b44+62964]\n",
      "\t(No symbol) [0x0x7710f3]\n",
      "\t(No symbol) [0x0x74ff59]\n",
      "\t(No symbol) [0x0x7e4f7e]\n",
      "\t(No symbol) [0x0x7ff6a9]\n",
      "\t(No symbol) [0x0x7de306]\n",
      "\t(No symbol) [0x0x7ad670]\n",
      "\t(No symbol) [0x0x7ae4e4]\n",
      "\tGetHandleVerifier [0x0xba4793+2556483]\n",
      "\tGetHandleVerifier [0x0xb9fd02+2537394]\n",
      "\tGetHandleVerifier [0x0x96a2fa+220586]\n",
      "\tGetHandleVerifier [0x0x95aae8+157080]\n",
      "\tGetHandleVerifier [0x0x96141d+184013]\n",
      "\tGetHandleVerifier [0x0x94ba68+95512]\n",
      "\tGetHandleVerifier [0x0x94bc10+95936]\n",
      "\tGetHandleVerifier [0x0x936b5a+9738]\n",
      "\tBaseThreadInitThunk [0x0x76075d49+25]\n",
      "\tRtlInitializeExceptionChain [0x0x7780d09b+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x0x7780d021+561]\n",
      "\n",
      "=================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeoutError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\envs\\MDS\\Lib\\socket.py:848\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    847\u001b[39m     sock.bind(source_address)\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mTimeoutError\u001b[39m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 104\u001b[39m\n\u001b[32m    102\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=================================================\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m             \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=================================================\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Save the results\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\envs\\MDS\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:224\u001b[39m, in \u001b[36mChromiumDriver.quit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mservice\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\envs\\MDS\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:150\u001b[39m, in \u001b[36mService.stop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process.poll() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_remote_shutdown_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    152\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\envs\\MDS\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:135\u001b[39m, in \u001b[36mService.send_remote_shutdown_command\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m30\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_connectable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    136\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    137\u001b[39m     sleep(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\envs\\MDS\\Lib\\site-packages\\selenium\\webdriver\\common\\service.py:124\u001b[39m, in \u001b[36mService.is_connectable\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_connectable\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    122\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Establishes a socket connection to determine if the service running\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[33;03m    on the port is accessible.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_connectable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\envs\\MDS\\Lib\\site-packages\\selenium\\webdriver\\common\\utils.py:98\u001b[39m, in \u001b[36mis_connectable\u001b[39m\u001b[34m(port, host)\u001b[39m\n\u001b[32m     96\u001b[39m socket_ = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     socket_ = \u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m     result = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _is_connectable_exceptions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\envs\\MDS\\Lib\\socket.py:855\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    853\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m error \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    854\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_errors:\n\u001b[32m--> \u001b[39m\u001b[32m855\u001b[39m         \u001b[43mexceptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# raise only the last error\u001b[39;00m\n\u001b[32m    856\u001b[39m     exceptions.append(exc)\n\u001b[32m    857\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "url = \"https://reg.bom.gov.au/climate/data/\"\n",
    "\n",
    "weather_types = ['Rainfall', 'Temp_Max', 'Temp_Min', 'Solar exposure']\n",
    "australian_regions = ['NSW', 'VIC', 'QLD', 'WA', 'SA', 'TAS', 'ACT', 'NT']\n",
    "australian_citys = ['Sydney', 'Melbourne', 'Brisbane', 'Perth', 'Adelaide', 'Hobart', 'Canberra', 'Darwin']\n",
    "\n",
    "zip_url = {}\n",
    "\n",
    "for weather_type in weather_types:\n",
    "    for region, city in zip(australian_regions, australian_citys):\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            wait = WebDriverWait(driver, 20)\n",
    "\n",
    "            # Select weather type\n",
    "            data_about_dropdown = wait.until(EC.presence_of_element_located((By.ID, 'ncc_obs_code_group')))\n",
    "            dropdown_xpath = {\n",
    "                'Rainfall': \"Rainfall\",\n",
    "                'Temp_Max': \"Temperature\",\n",
    "                'Temp_Min': \"Temperature\",\n",
    "                'Solar exposure': \"Solar exposure\"\n",
    "            }\n",
    "            data_about_dropdown.find_element(By.XPATH, f\"//option[text()='{dropdown_xpath[weather_type]}']\").click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Select sub-option for Temperature\n",
    "            if weather_type in ['Temp_Max', 'Temp_Min']:\n",
    "                wait.until(EC.visibility_of_element_located((By.ID, 'elementSubSelectLine')))\n",
    "                element_select = wait.until(EC.presence_of_element_located((By.ID, 'elementSubSelect')))\n",
    "                option_text = 'Maximum temperature' if weather_type == 'Temp_Max' else 'Minimum temperature'\n",
    "                element_select.find_element(By.XPATH, f\"//option[text()='{option_text}']\").click()\n",
    "                time.sleep(1)\n",
    "\n",
    "            # Input city\n",
    "            location_input = wait.until(EC.presence_of_element_located((By.ID, 'p_locSearch')))\n",
    "            location_input.clear()\n",
    "            location_input.send_keys(city)\n",
    "\n",
    "            # Click Find button and wait for match list\n",
    "            driver.find_element(By.ID, 'text').click()\n",
    "            wait.until(EC.presence_of_element_located((By.ID, 'matchList')))\n",
    "            time.sleep(1)\n",
    "            match_list = driver.find_element(By.ID, 'matchList')\n",
    "            match_list.find_elements(By.TAG_NAME, 'option')[0].click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Show open stations only\n",
    "            open_station_checkbox = wait.until(EC.element_to_be_clickable((By.ID, 'openStation')))\n",
    "            if not open_station_checkbox.is_selected():\n",
    "                open_station_checkbox.click()\n",
    "                time.sleep(2)  # Wait for list to refresh\n",
    "\n",
    "            # Select first station\n",
    "            nearest_stations = wait.until(EC.presence_of_element_located((By.ID, 'nearest10')))\n",
    "            station_options = nearest_stations.find_elements(By.TAG_NAME, 'option')\n",
    "            if len(station_options) == 0:\n",
    "                print(f'!!! No stations found for {city} - {weather_type}')\n",
    "                continue\n",
    "\n",
    "            station_options[0].click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            # Wait for station number to load\n",
    "            wait.until(lambda d: d.find_element(By.ID, 'p_stn_num').get_attribute('value').strip() != '')\n",
    "\n",
    "            # Get data\n",
    "            get_data_button = wait.until(EC.element_to_be_clickable((By.ID, 'getData')))\n",
    "            get_data_button.click()\n",
    "\n",
    "            # Wait and switch to new tab\n",
    "            wait.until(lambda d: len(d.window_handles) > 1)\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Wait for downloads section\n",
    "            wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'downloads')))\n",
    "            all_years_link = driver.find_element(By.LINK_TEXT, 'All years of data')\n",
    "            href = all_years_link.get_attribute('href')\n",
    "\n",
    "            # Save link\n",
    "            zip_url_key = f'{region}_{city}_{weather_type}'\n",
    "            zip_url[zip_url_key] = href\n",
    "            print(f'Collected ZIP URL for {city} - {weather_type}: {href}')\n",
    "\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {city} - {weather_type}: {e}')\n",
    "            print(\"=================================================\")\n",
    "        finally:\n",
    "            driver.quit()\n",
    "            print(\"=================================================\")\n",
    "\n",
    "# Save the results\n",
    "print(\"\\nAll collected ZIP URLs:\")\n",
    "for key, link in zip_url.items():\n",
    "    print(f'{key}: {link}')\n",
    "\n",
    "df_urls = pd.DataFrame(list(zip_url.items()), columns=['Region_City_WeatherType', 'ZIP_URL'])\n",
    "df_urls.to_csv('./data/AUS_weather/zip_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c274975",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53210b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "# Load ZIP URLs from CSV\n",
    "df_urls = pd.read_csv('./data/AUS_weather/zip_urls.csv')\n",
    "dict_zip_urls = dict(zip(df_urls['Region_City_WeatherType'], df_urls['ZIP_URL']))\n",
    "\n",
    "# Group keys by Region_City prefix (e.g., NSW_Sydney)\n",
    "city_groups = {}\n",
    "for key in dict_zip_urls.keys():\n",
    "    city = '_'.join(key.split('_')[:2])\n",
    "    if city not in city_groups:\n",
    "        city_groups[city] = []\n",
    "    city_groups[city].append(key)\n",
    "\n",
    "# Create output folder\n",
    "os.makedirs('./data/AUS_weather/merged_per_city', exist_ok=True)\n",
    "\n",
    "# Iterate over each city\n",
    "for city, keys in city_groups.items():\n",
    "    print(f\"\\n Processing: {city}\")\n",
    "\n",
    "    dfs = []\n",
    "    for key in keys:\n",
    "        try:\n",
    "            url = dict_zip_urls[key]\n",
    "            print(f\"  ⬇️ Downloading: {key}\")\n",
    "            response = requests.get(url)\n",
    "            print(f\"    Content-Type: {response.headers.get('Content-Type')}\")\n",
    "\n",
    "            # Save the ZIP file to disk\n",
    "            zip_filename = f\"{key}.zip\"\n",
    "            with open(zip_filename, 'wb') as temp_file:\n",
    "                temp_file.write(response.content)\n",
    "            print(f\"    Saved {zip_filename} for inspection.\")\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"    !!! Failed to download {key}\")\n",
    "                continue\n",
    "\n",
    "            # Extract the CSV file from the ZIP\n",
    "            with zipfile.ZipFile(zip_filename, 'r') as z:\n",
    "                csv_name = z.namelist()[0]\n",
    "                z.extract(csv_name, './data/AUS_weather/merged_per_city')\n",
    "                print(f\"    Extracted {csv_name} to ./data/AUS_weather/merged_per_city\")\n",
    "\n",
    "                # Read the CSV into a DataFrame\n",
    "                csv_path = os.path.join('./data/AUS_weather/merged_per_city', csv_name)\n",
    "                df = pd.read_csv(csv_path)\n",
    "\n",
    "            # Remove the ZIP file\n",
    "            os.remove(zip_filename)\n",
    "            print(f\"    Deleted ZIP file: {zip_filename}\")\n",
    "\n",
    "            # Optionally, remove the extracted CSV after processing\n",
    "            os.remove(csv_path)\n",
    "\n",
    "            # ...rest of your processing...\n",
    "            df.drop(columns=['Product code', 'Bureau of Meteorology station number'], errors='ignore', inplace=True)\n",
    "            base_cols = ['Year', 'Month', 'Day']\n",
    "            weather_cols = [col for col in df.columns if col not in base_cols]\n",
    "            df = df[base_cols + weather_cols]\n",
    "            weather_type = '_'.join(key.split('_')[2:])\n",
    "            df.rename(columns={col: f\"{weather_type}_{col}\" for col in weather_cols}, inplace=True)\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    !!! Error processing {key}: {e}\")\n",
    "\n",
    "    # Merge all weather types on Year-Month-Day\n",
    "    if dfs:\n",
    "        merged_df = dfs[0]\n",
    "        for other_df in dfs[1:]:\n",
    "            merged_df = pd.merge(merged_df, other_df, on=['Year', 'Month', 'Day'], how='outer')\n",
    "\n",
    "        # Add 'location' column\n",
    "        merged_df['location'] = city\n",
    "\n",
    "        # Save merged file\n",
    "        merged_df.to_csv(f'./data/AUS_weather/merged_per_city/{city}_merged.csv', index=False)\n",
    "        print(f\" Saved: {city}_merged.csv\")\n",
    "        print(\"==================================================\")\n",
    "    else:\n",
    "        print(f\"!!! Skipping {city}, no valid data.\")\n",
    "        print(\"==================================================\")\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "target_path = os.path.join(current_dir, 'data', 'AUS_weather', 'merged_per_city')\n",
    "\n",
    "# concatenate all csv files into one dataframe except one csv file called Australia_disaster-mapper-data-21-03-2023.csv\n",
    "all_files = [f for f in os.listdir(target_path) if f.endswith('.csv') and f != 'Australia_disaster-mapper-data-21-03-2023.csv']\n",
    "dfs = []\n",
    "for file in all_files:\n",
    "    file_path = os.path.join(target_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv(os.path.join(target_path, 'AUS_bigcity_weather_data.csv'), index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00dbab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping row 0 due to missing Start Date and End Date\n",
      "Skipping row 75 due to missing Start Date and End Date\n",
      "Skipping row 97 due to missing Start Date and End Date\n",
      "Skipping row 143 due to missing Start Date and End Date\n",
      "Skipping row 154 due to missing Start Date and End Date\n",
      "Skipping row 169 due to missing Start Date and End Date\n",
      "Skipping row 195 due to missing Start Date and End Date\n",
      "Skipping row 285 due to missing Start Date and End Date\n",
      "Skipping row 340 due to missing Start Date and End Date\n",
      "Skipping row 345 due to missing Start Date and End Date\n",
      "Skipping row 348 due to missing Start Date and End Date\n",
      "Skipping row 424 due to missing Start Date and End Date\n",
      "Skipping row 444 due to missing Start Date and End Date\n",
      "Skipping row 453 due to missing Start Date and End Date\n",
      "Skipping row 590 due to missing Start Date and End Date\n",
      "Skipping row 593 due to missing Start Date and End Date\n",
      "Skipping row 707 due to missing Start Date and End Date\n",
      "Skipping row 710 due to missing Start Date and End Date\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "target_path = os.path.join(current_dir, 'data', 'AUS_weather', 'merged_per_city')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# australian_regions = ['NSW', 'VIC', 'QLD', 'WA', 'SA', 'TAS', 'ACT', 'NT']\n",
    "# australian_citys = ['Sydney', 'Melbourne', 'Brisbane', 'Perth', 'Adelaide', 'Hobart', 'Canberra', 'Darwin']\n",
    "\n",
    "# region_retrival\n",
    "zone_retrival = {\n",
    "    'NSW': 'New South Wales',\n",
    "    'VIC': 'Victoria',\n",
    "    'QLD': 'Queensland',\n",
    "    'WA': 'Western Australia',\n",
    "    'SA': 'South Australia',\n",
    "    'TAS': 'Tasmania',\n",
    "    'ACT': 'Australian Capital Territory',\n",
    "    'NT': 'Northern Territory'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "normal_weather_df = pd.read_csv(os.path.join(target_path, 'AUS_bigcity_weather_data.csv'))\n",
    "\n",
    "# merge Year, Month, Day columns into a single datetime column\n",
    "normal_weather_df['Date'] = pd.to_datetime(normal_weather_df[['Year', 'Month', 'Day']]).apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "normal_weather_df['Date'] = pd.to_datetime(normal_weather_df['Date'], format='%Y-%m-%d', errors='coerce')\n",
    "# Drop the original Year, Month, Day columns\n",
    "normal_weather_df.drop(columns=['Year', 'Month', 'Day'], inplace=True)\n",
    "# Reorder columns to have 'Date' first\n",
    "normal_weather_df = normal_weather_df[['Date'] + [col for col in normal_weather_df.columns if col != 'Date']]\n",
    "\n",
    "\n",
    "normal_weather_df[\"Extreme_weather\"] = \"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the extreme weather data\n",
    "extreme_weather_df = pd.read_csv(os.path.join(target_path, 'Australia_disaster-mapper-data-21-03-2023.csv'))\n",
    "\n",
    "# Convert to datetime (keep as datetime)\n",
    "extreme_weather_df['Start Date'] = pd.to_datetime(extreme_weather_df['Start Date'], errors='coerce', dayfirst=True)\n",
    "extreme_weather_df['End Date'] = pd.to_datetime(extreme_weather_df['End Date'], errors='coerce', dayfirst=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for index, row in extreme_weather_df.iterrows():\n",
    "    start_date = row['Start Date']\n",
    "    end_date = row['End Date']\n",
    "\n",
    "    if pd.isna(start_date) or pd.isna(end_date):\n",
    "\n",
    "        if pd.isna(start_date) and pd.isna(end_date):\n",
    "            print(f\"Skipping row {index} due to missing Start Date and End Date\")\n",
    "            continue\n",
    "\n",
    "        if pd.notna(start_date):\n",
    "            end_date = start_date + timedelta(days=10)\n",
    "\n",
    "        if pd.notna(end_date):\n",
    "            start_date = end_date - timedelta(days=10)\n",
    "\n",
    "    # Split multiple regions and clean\n",
    "    regions = [r.strip() for r in str(row['Zone']).split(',')]\n",
    "    category = row['Category']\n",
    "    if category == 'Environment':\n",
    "        category = 'Drought'\n",
    "    elif category == 'Health':\n",
    "        category = 'Heatwave'\n",
    "    elif category == 'Industrial':\n",
    "        category = 'Industrial Accident'\n",
    "    elif category == 'Maritime/Coastal':\n",
    "        category = 'Shipwreck'\n",
    "    elif category == 'Transport':\n",
    "        category = 'Transport Accident'\n",
    "\n",
    "    \n",
    "\n",
    "    # Check if location's region matches ANY of the regions in the list\n",
    "    region_mask = normal_weather_df['location'].apply(\n",
    "        lambda x: zone_retrival[x.split('_')[0].strip()] in regions\n",
    "    )\n",
    "\n",
    "    # Select records matching region and date range (dates are already datetime)\n",
    "    mask = region_mask & (normal_weather_df['Date'] >= start_date) & (normal_weather_df['Date'] <= end_date)\n",
    "\n",
    "    # If there's already a category, append the new one\n",
    "    normal_weather_df.loc[mask, 'Extreme_weather'] = normal_weather_df.loc[mask, 'Extreme_weather'].apply(\n",
    "        lambda x: f\"{x} | {category}\" if x else category\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Format dates only when exporting\n",
    "extreme_weather_df['Start Date'] = extreme_weather_df['Start Date'].dt.strftime('%Y-%m-%d')\n",
    "extreme_weather_df['End Date'] = extreme_weather_df['End Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "normal_weather_df.to_csv(os.path.join(target_path, 'AUS_bigcity_weather_data_code_cleaned.csv'), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78036975",
   "metadata": {},
   "source": [
    "# Nature Language Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe5648be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------------------- -------------------- 6.3/12.8 MB 38.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 33.5 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31e458a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')  # For tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3897e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import emoji\n",
    "import emot\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "# import torch  # if you want to use token IDs later\n",
    "\n",
    "class NLP_OPERATORS:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=[\"parser\", \"ner\", \"textcat\"])\n",
    "        self.nlp.max_length = 5_000_000\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def convert_emojis_emoticons(self, text):\n",
    "        e = emot.core.emot()\n",
    "        emoticon_results = e.emoticons(text)\n",
    "        for original, meaning in zip(emoticon_results['value'], emoticon_results['mean']):\n",
    "            text = text.replace(original, f\" {meaning} \")\n",
    "\n",
    "        # # Extract emojis\n",
    "        # emoji_results = e.emoji(text)\n",
    "        # for original, meaning in zip(emoji_results['value'], emoji_results['mean']):\n",
    "        #     text = text.replace(original, f\" {meaning} \")\n",
    "\n",
    "\n",
    "        # Convert emojis to text\n",
    "        text = emoji.demojize(text)\n",
    "\n",
    "\n",
    "        # Make BERT-compatible\n",
    "        text = text.replace(\":\", \" \").replace(\"_\", \" \")  \n",
    "        return text.strip().lower()\n",
    "        \n",
    "\n",
    "    def basic_cleaning(self, text):\n",
    "        text = self.convert_emojis_emoticons(text)\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "        text = re.sub(r'https?://\\S+', '', text)\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'&quot;', '', text)\n",
    "        text = re.sub(r'</?.*?>', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def text_preprocessing(self, text, regex=False, remove_stop_word=False, lemmatisation=False, lower_case=False,\n",
    "                           return_tokens=True, use_bert_tokenizer=False):\n",
    "        \n",
    "        if use_bert_tokenizer:\n",
    "\n",
    "            text = self.basic_cleaning(text)\n",
    "            tokens = self.bert_tokenizer.tokenize(text)\n",
    "            return tokens if return_tokens else \" \".join(tokens)\n",
    "        \n",
    "\n",
    "        if regex:\n",
    "            text = self.basic_cleaning(text)\n",
    "\n",
    "\n",
    "        if lower_case:\n",
    "            text = text.lower()\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        if remove_stop_word:\n",
    "            tokens = [word for word in tokens if word not in self.stop_words]\n",
    "\n",
    "        if lemmatisation:\n",
    "            doc = self.nlp(' '.join(tokens))\n",
    "            tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "        return tokens if return_tokens else \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a34477",
   "metadata": {},
   "source": [
    "preprocess for statistical model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f2529",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "clean_operator = NLP_OPERATORS()\n",
    "\n",
    "# get the current path\n",
    "current_path = os.getcwd()\n",
    "output_folder = os.path.join(current_path, 'data', 'reddit_data', 'clean')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# read the csv file\n",
    "df_mental = pd.read_csv(os.path.join(current_path, 'data', 'reddit_data', 'mental_reddit_posts.csv'))\n",
    "# print(df_mental.shape)\n",
    "\n",
    "\n",
    "\n",
    "df_mental[\"title_selftext_topcomments_text\"] = df_mental[\"title\"].astype(str) + \" | \" + df_mental[\"selftext\"].astype(str) + \" | \" + df_mental[\"top_comments\"].astype(str)\n",
    "df_mental['clean_title_selftext_topcomments_text'] = df_mental['title_selftext_topcomments_text'].copy().apply(\n",
    "    lambda x: clean_operator.text_preprocessing(x, regex=True, remove_stop_word=True, lemmatisation=True, lower_case=True, return_tokens=False))\n",
    "\n",
    "# combine all the text in the column into a single string\n",
    "clean_all_strings = \" \".join(df_mental['clean_title_selftext_topcomments_text'].tolist())\n",
    "token_clean_all_strings = clean_operator.text_preprocessing(clean_all_strings, return_tokens=True)\n",
    "\n",
    "# save this into a text file\n",
    "with open(os.path.join(output_folder, 'token_cleaned_mental_all_text.txt'), 'w') as f:\n",
    "    f.write(str(token_clean_all_strings))\n",
    "\n",
    "#save the df_mental into a csv file\n",
    "df_mental.to_csv(os.path.join(output_folder, 'cleaned_mental_all_text.csv'), index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_normal = pd.read_csv(os.path.join(current_path, 'data', 'reddit_data', 'normal_reddit_posts.csv'))\n",
    "# print(df_normal.shape)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb071b6",
   "metadata": {},
   "source": [
    "preprocess for deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3013d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# use bert tokenizer to tokenize the text \n",
    "not_clean_all_strings = \" \".join(df_mental['title_selftext_topcomments_text'].tolist())\n",
    "bert_token_not_clean_all_strings = clean_operator.text_preprocessing(not_clean_all_strings,use_bert_tokenizer = True, return_tokens=True)\n",
    "\n",
    "# save it to a pickle file\n",
    "import pickle\n",
    "with open(os.path.join(output_folder, 'bert_token_not_clean_all_text.pkl'), 'wb') as f:\n",
    "    pickle.dump(bert_token_not_clean_all_strings, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d18ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clean_operator = NLP_OPERATORS()\n",
    "\n",
    "\n",
    "\n",
    "df_anxiety = pd.read_csv('./data/beyondblue_data/Anxiety_beyondblue_posts.csv')\n",
    "print(df_anxiety.shape)\n",
    "\n",
    "df_depression = pd.read_csv('./data/beyondblue_data/Depression_beyondblue_posts.csv')\n",
    "print(df_depression.shape)\n",
    "\n",
    "df_ptsd = pd.read_csv('./data/beyondblue_data/PTSD_beyondblue_posts.csv')\n",
    "print(df_ptsd.shape)\n",
    "\n",
    "df_suicide_selfharm = pd.read_csv('./data/beyondblue_data/Suicide_selfharm_beyondblue_posts.csv')\n",
    "print(df_suicide_selfharm.shape)\n",
    "\n",
    "df_Staying_well = pd.read_csv('./data/beyondblue_data/Staying_well_beyondblue_posts.csv')\n",
    "print(df_Staying_well.shape)\n",
    "\n",
    "df_treament = pd.read_csv('./data/beyondblue_data/Treament_beyondblue_posts.csv')\n",
    "print(df_treament.shape)\n",
    "\n",
    "df_relationship = pd.read_csv('./data/beyondblue_data/Relationship_family_issues_beyondblue_posts.csv')\n",
    "print(df_relationship.shape)\n",
    "\n",
    "df_youth = pd.read_csv('./data/beyondblue_data/Youth_beyondblue_posts.csv')\n",
    "print(df_youth.shape)\n",
    "\n",
    "df_sex = pd.read_csv('./data/beyondblue_data/Sex_identity_beyondblue_posts.csv')\n",
    "print(df_sex.shape)\n",
    "\n",
    "df_multiculture = pd.read_csv('./data/beyondblue_data/Multiculture_beyondblue_posts.csv')\n",
    "print(df_multiculture.shape)\n",
    "      \n",
    "df_grief = pd.read_csv('./data/beyondblue_data/Grief_loss_beyondblue_posts.csv')\n",
    "print(df_grief.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# read all the csv files in beyondblue_data repository and concatenate them into a single dataframe\n",
    "df_beyondblue = pd.DataFrame()\n",
    "beyondblue_data_path = os.path.join(current_path, 'data', 'beyondblue_data')\n",
    "for file in os.listdir(beyondblue_data_path):\n",
    "    if file.endswith('.csv'):\n",
    "        temp_df = pd.read_csv(os.path.join(beyondblue_data_path, file))\n",
    "        df_beyondblue = pd.concat([df_beyondblue, temp_df], ignore_index=True)\n",
    "# remove duplicates by Post ID\n",
    "df_beyondblue = df_beyondblue.drop_duplicates(subset='Post ID').reset_index(drop=True)\n",
    "# Convert 'Post Date' to datetime format\n",
    "df_beyondblue['Post Date'] = pd.to_datetime(df_beyondblue['Post Date'], errors='coerce')\n",
    "# Sort by 'Post Date' in descending order\n",
    "df_beyondblue = df_beyondblue.sort_values(by='Post Date', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(df_beyondblue.shape)\n",
    "\n",
    "df_beyondblue['preprocessed_token_post_title'] = df_beyondblue['Post Title'].apply(lambda x: clean_operator.text_preprocessing(x, regex=True, remove_stop_word=True, lemmatisation=True, lower_case=True, return_tokens=True))\n",
    "df_beyondblue['preprocessed_token_post_content'] = df_beyondblue['Post Content'].apply(lambda x: clean_operator.text_preprocessing(x, regex=True, remove_stop_word=True, lemmatisation=True, lower_case=True, return_tokens=True))\n",
    "df_beyondblue['preprocessed_token_comments'] = df_beyondblue['Comments'].apply(lambda x: clean_operator.text_preprocessing(x, regex=True, remove_stop_word=True, lemmatisation=True, lower_case=True, return_tokens=True))\n",
    "df_beyondblue.to_csv(os.path.join(current_path, 'data', 'beyondblue_data','clean', 'clean_beyondblue_posts.csv'), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4351b6e",
   "metadata": {},
   "source": [
    "feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a60cc47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fa5bb6b",
   "metadata": {},
   "source": [
    "# CATEGORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_symptom_categories = [\n",
    "    \"Depression\",\n",
    "    \"Anxiety\",\n",
    "    \"Stress/Burnout\",\n",
    "    \"Loneliness\",\n",
    "    \"Low Self-Esteem\",\n",
    "    \"Trauma/PTSD\",\n",
    "    \"Anger/Irritability\",\n",
    "    \"Obsessive Thoughts\",\n",
    "    \"Addiction\"\n",
    "]\n",
    "\n",
    "suicide_and_selfharm_labels = [\n",
    "    \"Suicidal Ideation\",\n",
    "    \"Suicide Attempt\",\n",
    "    \"Self-Harm\",\n",
    "    \"Despair\",\n",
    "    \"Urgent Help Request\",\n",
    "    \"Grief After Suicide\",\n",
    "    \"Coping with Suicidal Thoughts\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b75e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e969747d",
   "metadata": {},
   "source": [
    "# NETWORK VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be470570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0416cd9",
   "metadata": {},
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c2973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
