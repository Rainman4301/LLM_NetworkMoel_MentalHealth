{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620d51aa",
   "metadata": {},
   "source": [
    "# Mental Health Analysis ProjectA \n",
    "\n",
    "The purpose of this project is ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa92f41",
   "metadata": {},
   "source": [
    "# Data Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1710d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_symptom_categories = [\n",
    "    \"Depression\",\n",
    "    \"Anxiety\",\n",
    "    \"Stress/Burnout\",\n",
    "    \"Loneliness\",\n",
    "    \"Low Self-Esteem\",\n",
    "    \"Trauma/PTSD\",\n",
    "    \"Anger/Irritability\",\n",
    "    \"Obsessive Thoughts\",\n",
    "    \"Addiction\"\n",
    "]\n",
    "\n",
    "suicide_and_selfharm_labels = [\n",
    "    \"Suicidal Ideation\",\n",
    "    \"Suicide Attempt\",\n",
    "    \"Self-Harm\",\n",
    "    \"Despair\",\n",
    "    \"Urgent Help Request\",\n",
    "    \"Grief After Suicide\",\n",
    "    \"Coping with Suicidal Thoughts\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bbce6f",
   "metadata": {},
   "source": [
    "Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping data from reddit api\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import praw\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RedditScraper:\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.auth = requests.auth.HTTPBasicAuth(os.getenv('CLIENT_ID'), os.getenv('CLIENT_SECRET'))\n",
    "        self.data = {'grant_type': 'password',\n",
    "                     'username': os.getenv('USERNAME'),\n",
    "                     'password': os.getenv('PASSWORD')}\n",
    "        self.headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "        self.res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                                auth=self.auth, data=self.data, headers=self.headers)\n",
    "        \n",
    "        self.headers[\"Authorization\"] = f'bearer {self.res.json()[\"access_token\"]}'\n",
    "\n",
    "    def get_posts_byrequests(self, subreddit, limit=1000):\n",
    "        url = f'https://oauth.reddit.com/r/{subreddit}/hot'\n",
    "        params = {'limit': limit}\n",
    "        response = requests.get(url, headers=self.headers, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        else:\n",
    "            raise Exception(f\"Error fetching posts: {response.status_code} - {response.text}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_posts_byprawn(self, subreddits, limit=1000, mental=\"mental_\"):\n",
    "\n",
    "        reddit = praw.Reddit(client_id=os.getenv('CLIENT_ID'),\n",
    "                             client_secret=os.getenv('CLIENT_SECRET'),\n",
    "                             user_agent='windows:mentalhealth.scraper:v1.0 (by u/IceWorth5480)',\n",
    "                             username=os.getenv('USERNAME'),\n",
    "                             password=os.getenv('PASSWORD'))\n",
    "        \n",
    "        all_posts = []\n",
    "        sort_types = ['hot', 'top', 'new']\n",
    "\n",
    "\n",
    "        for subreddit in subreddits:\n",
    "            for sort in sort_types:\n",
    "                if sort == 'hot':\n",
    "                    posts = reddit.subreddit(subreddit).hot(limit=limit)\n",
    "                elif sort == 'top':\n",
    "                    posts = reddit.subreddit(subreddit).top(limit=limit)\n",
    "                elif sort == 'new':\n",
    "                    posts = reddit.subreddit(subreddit).new(limit=limit)\n",
    "\n",
    "                for post in posts:\n",
    "                    if post is None:\n",
    "                        continue\n",
    "                    all_posts.append({\n",
    "                        'id': post.id,\n",
    "                        'subreddit': subreddit,\n",
    "                        'sort': sort,  # Add sort type for tracking\n",
    "                        'title': post.title,\n",
    "                        'selftext': post.selftext,\n",
    "                        'created_utc': post.created_utc,\n",
    "                        'score': post.score,\n",
    "                        'num_comments': post.num_comments,\n",
    "                        'author': str(post.author),\n",
    "                        'url': post.url,\n",
    "                        'over_18': post.over_18,\n",
    "                        'flair': post.link_flair_text\n",
    "                    })\n",
    "\n",
    "\n",
    "        # delete dupicates by id\n",
    "        all_posts = [dict(t) for t in {tuple(d.items()) for d in all_posts}]\n",
    " \n",
    "\n",
    "\n",
    "        df = pd.DataFrame(all_posts)\n",
    "        df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "        df = df.sort_values(by='created_utc', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        # check if f'./data/reddit_data/{mental}reddit_posts.csv' exists, if so merging with df and # delete dupicates by id\n",
    "        if os.path.exists(f'./data/reddit_data/{mental}reddit_posts.csv'):\n",
    "            existing_df = pd.read_csv(f'./data/reddit_data/{mental}reddit_posts.csv')\n",
    "            df = pd.concat([existing_df, df]).drop_duplicates(subset='id').reset_index(drop=True)\n",
    "       \n",
    "        # save as csv \n",
    "        df.to_csv(f'./data/reddit_data/{mental}reddit_posts.csv', index=False)\n",
    "\n",
    "\n",
    "        \n",
    "    def get_posts_byPushshift(self, subreddit, limit=1000):\n",
    "        url = f'https://api.pushshift.io/reddit/search/submission/?subreddit={subreddit}&sort=desc&limit={limit}'\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        print(response.status_code)\n",
    "        print(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mental_subreddits = ['mentalhealth', 'depression', 'anxiety', 'therapy', 'selfhelp', 'bpd', 'ptsd', 'socialanxiety', 'counseling']\n",
    "normal_subreddits = ['popular','all','AskReddit','interestingasfuck']\n",
    "test = RedditScraper()\n",
    "\n",
    "test.get_posts_byprawn(mental_subreddits, limit=1000,mental=\"mental_\")\n",
    "test.get_posts_byprawn(normal_subreddits, limit=1000, mental=\"normal_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a6a469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15877, 12)\n",
      "(6878, 12)\n"
     ]
    }
   ],
   "source": [
    "# read the csv file \n",
    "\n",
    "df_mental = pd.read_csv('./data/reddit_data/mental_reddit_posts.csv')\n",
    "df_normal = pd.read_csv('./data/reddit_data/normal_reddit_posts.csv')\n",
    "\n",
    "print(df_mental.shape)\n",
    "print(df_normal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5613c",
   "metadata": {},
   "source": [
    "Beyond Blue forums "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TO DO\n",
    "\n",
    "Extract following information from the reddit webpage \n",
    "\n",
    "\n",
    "Post ID: A unique identifier for each post.\n",
    "Post Content: The text of the post.\n",
    "Post Author: The author of the post.\n",
    "Post Date: The date the post was made.\n",
    "Post Category: Category or forum where the post was made.\n",
    "Number of Comments: The total number of comments on the post.\n",
    "\n",
    "From Comment \n",
    "\n",
    "Post ID: Link back to the original post.\n",
    "Comment ID: A unique identifier for each comment.\n",
    "Comment Content: Text of the comment.\n",
    "Comment Author: Author of the comment.\n",
    "Comment Date: Date the comment was posted. (the order of the comments is really important)\n",
    "other meta data if available\n",
    "'''\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_post_date(raw_date):\n",
    "\n",
    "    weekdays = list(calendar.day_name)\n",
    "    today = datetime.now()\n",
    "\n",
    "    raw_date = raw_date.strip()\n",
    "\n",
    "    if raw_date in weekdays:\n",
    "        # Get weekday index: Monday = 0, Sunday = 6\n",
    "        post_weekday_index = weekdays.index(raw_date)\n",
    "        today_weekday_index = today.weekday()\n",
    "\n",
    "        # Calculate difference in days\n",
    "        delta_days = (today_weekday_index - post_weekday_index) % 7\n",
    "\n",
    "        # Get actual date\n",
    "        post_date = today - timedelta(days=delta_days)\n",
    "        return post_date.strftime('%Y-%m-%d')  # Format as YYYY-MM-DD\n",
    "\n",
    "    else:\n",
    "        # Try parsing date in the format like \"25-09-2020\"\n",
    "        try:\n",
    "            post_date = datetime.strptime(raw_date, '%d-%m-%Y')\n",
    "            return post_date.strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            return 'Unknown date'  # If format is unexpected\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# post_variables = [\"Post ID\", \"Post Content\", \"Post Author\", \"Post Date\", \"Post Category\", \"Number of Comments\"]\n",
    "mental_health_type =['Anxiety','Depression','PTSD and trauma','Suicidal thoughts and self-harm']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def beyondblue_scrapping(tag,address):\n",
    "\n",
    "\n",
    "    # Setup Chrome WebDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    url = address\n",
    "\n",
    "    whole_data = []\n",
    "\n",
    "\n",
    "\n",
    "    for page in tqdm(range(1, 20), desc=\"Scraping pages\"):\n",
    "\n",
    "    \n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "\n",
    "        # get the class \"custom-message-list all-discussions\"\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "\n",
    "        discussions = soup.find('div', class_='custom-message-list all-discussions').find_all(('article'))\n",
    "        \n",
    "\n",
    "        for post in discussions:\n",
    "\n",
    "            # Extracting post id\n",
    "            post_id = post.get('class')[-1] if post.get('class') else \"\"\n",
    "\n",
    "            # Extracting post title\n",
    "            title_tag = post.find_all('h3')[0].find_all('a')[1]\n",
    "            post_title = title_tag.text.strip() if title_tag else \"\"\n",
    "\n",
    "            # Extracting post content\n",
    "            post_content = post.find('p', class_ = 'body-text').text.strip() if post.find('p', class_ = 'body-text') else \"\"\n",
    "            \n",
    "\n",
    "            side_info = post.find('aside')\n",
    "            side_info1 = side_info.find('div', class_='custom-tile-author-info') if side_info else None\n",
    "            # Extracting post author\n",
    "            post_author = side_info1.find('a').find('span').text.strip() if side_info and side_info.find('span') else \"\"\n",
    "\n",
    "\n",
    "            side_info2 = side_info.find('div', class_='custom-tile-category-content')\n",
    "            # Extracting post tag\n",
    "            post_tag = side_info2.find('a').text.strip() if side_info2 and side_info2.find('a') else \"\"\n",
    "            \n",
    "            # Extracting post date\n",
    "            raw_date = side_info2.find('time').text.strip() if side_info2 and side_info2.find('time') else \"\"\n",
    "            post_date = parse_post_date(raw_date)\n",
    "\n",
    "\n",
    "\n",
    "            side_info3 = side_info.find('div', class_='custom-tile-unread-replies')\n",
    "            unread = side_info3.find('span').text.strip() if side_info3 and side_info3.find('span') else \"\"\n",
    "            # Extracting number of unread replies\n",
    "            match = re.search(r'\\d+', unread)\n",
    "            post_unread = int(match.group()) if match else 0\n",
    "\n",
    "            # Extracting number of comments\n",
    "            post_comments = post.find('li', class_ = 'custom-tile-replies').find('b').text.strip() if post.find('li', class_ = 'custom-tile-replies') else \"\"\n",
    "\n",
    "            post_data = {\n",
    "                \"Post ID\": post_id,\n",
    "                \"Post Title\": post_title,\n",
    "                \"Post Content\": post_content,\n",
    "                \"Post Author\": post_author,\n",
    "                \"Post Date\": post_date,\n",
    "                \"Post Category\": post_tag,\n",
    "                \"Number of Comments\": post_comments\n",
    "            }\n",
    "            whole_data.append(post_data)\n",
    "\n",
    "\n",
    "        # Find next page link\n",
    "        next_page = soup.find('li', class_='lia-paging-page-next')\n",
    "        if next_page and next_page.find('a'):\n",
    "            url = next_page.find('a')['href']\n",
    "        else:\n",
    "            print(\"No more pages to scrape.\")\n",
    "            break\n",
    "\n",
    "    #close the driver\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(whole_data)\n",
    "    # Save to CSV\n",
    "    df.to_csv(f'./data/beyondblue_data/{tag}_beyondblue_posts.csv', index=False)\n",
    "    print(f\"Data saved to ./data/beyondblue_data/{tag}_beyondblue_posts.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50df711d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 4/4 [00:18<00:00,  4.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/beyondblue_data/Anxiety_beyondblue_posts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 4/4 [00:19<00:00,  4.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/beyondblue_data/Depression_beyondblue_posts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 4/4 [00:21<00:00,  5.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/beyondblue_data/PTSD_beyondblue_posts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 4/4 [00:20<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/beyondblue_data/Suicidal_thoughts_and_self-harm_beyondblue_posts.csv\n"
     ]
    }
   ],
   "source": [
    "mental_health_urls = {\n",
    "    \"Anxiety\": \"https://forums.beyondblue.org.au/t5/anxiety/bd-p/c1-sc2-b1?&sort=recent\",\n",
    "    \"Depression\": \"https://forums.beyondblue.org.au/t5/depression/bd-p/c1-sc2-b2?&sort=recent\",\n",
    "    \"PTSD\": \"https://forums.beyondblue.org.au/t5/ptsd-and-trauma/bd-p/c1-sc2-b3?&sort=recent\",\n",
    "    \"Suicidal_thoughts_and_self-harm\": \"https://forums.beyondblue.org.au/t5/suicidal-thoughts-and-self-harm/bd-p/c1-sc2-b4?&sort=recent\"\n",
    "}\n",
    "\n",
    "\n",
    "for tag, address in mental_health_urls.items():\n",
    "    try:\n",
    "        beyondblue_scrapping(tag, address)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {tag}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "087e68d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 7)\n",
      "(40, 7)\n",
      "(40, 7)\n",
      "(40, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the csv file\n",
    "df_anxiety = pd.read_csv('./data/beyondblue_data/Anxiety_beyondblue_posts.csv')\n",
    "df_depression = pd.read_csv('./data/beyondblue_data/Depression_beyondblue_posts.csv')\n",
    "df_ptsd = pd.read_csv('./data/beyondblue_data/PTSD_beyondblue_posts.csv')\n",
    "df_self_harm = pd.read_csv('./data/beyondblue_data/Suicidal_thoughts_and_self-harm_beyondblue_posts.csv')\n",
    "\n",
    "\n",
    "print(df_anxiety.shape)\n",
    "print(df_depression.shape)  \n",
    "print(df_ptsd.shape)\n",
    "print(df_self_harm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf93aa",
   "metadata": {},
   "source": [
    "Climate data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53210b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78036975",
   "metadata": {},
   "source": [
    "# Nature Language Process\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
