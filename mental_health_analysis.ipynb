{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620d51aa",
   "metadata": {},
   "source": [
    "# Mental Health Analysis ProjectA \n",
    "\n",
    "The purpose of this project is ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa92f41",
   "metadata": {},
   "source": [
    "# Data Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1710d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_symptom_categories = [\n",
    "    \"Depression\",\n",
    "    \"Anxiety\",\n",
    "    \"Stress/Burnout\",\n",
    "    \"Loneliness\",\n",
    "    \"Low Self-Esteem\",\n",
    "    \"Trauma/PTSD\",\n",
    "    \"Anger/Irritability\",\n",
    "    \"Obsessive Thoughts\",\n",
    "    \"Addiction\"\n",
    "]\n",
    "\n",
    "suicide_and_selfharm_labels = [\n",
    "    \"Suicidal Ideation\",\n",
    "    \"Suicide Attempt\",\n",
    "    \"Self-Harm\",\n",
    "    \"Despair\",\n",
    "    \"Urgent Help Request\",\n",
    "    \"Grief After Suicide\",\n",
    "    \"Coping with Suicidal Thoughts\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bbce6f",
   "metadata": {},
   "source": [
    "Reddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d39e0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping data from reddit api\n",
    "import requests\n",
    "import pandas as pd\n",
    "import praw\n",
    "import emoji\n",
    "import emot\n",
    "import asyncpraw\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RedditScraper:\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.auth = requests.auth.HTTPBasicAuth(os.getenv('CLIENT_ID'), os.getenv('CLIENT_SECRET'))\n",
    "        self.data = {'grant_type': 'password',\n",
    "                     'username': os.getenv('USERNAME'),\n",
    "                     'password': os.getenv('PASSWORD')}\n",
    "        self.headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "        self.res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                                auth=self.auth, data=self.data, headers=self.headers)\n",
    "        \n",
    "        self.headers[\"Authorization\"] = f'bearer {self.res.json()[\"access_token\"]}'\n",
    "\n",
    "\n",
    "\n",
    "        self.client_id = os.getenv('CLIENT_ID')\n",
    "        self.client_secret = os.getenv('CLIENT_SECRET')\n",
    "        self.username = os.getenv('USERNAME')\n",
    "        self.password = os.getenv('PASSWORD')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_posts_byrequests(self, subreddit, limit=1000):\n",
    "        url = f'https://oauth.reddit.com/r/{subreddit}/hot'\n",
    "        params = {'limit': limit}\n",
    "        response = requests.get(url, headers=self.headers, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        else:\n",
    "            raise Exception(f\"Error fetching posts: {response.status_code} - {response.text}\")\n",
    "        \n",
    "\n",
    "\n",
    "    def convert_emojis_emoticons(self, text):\n",
    "        e = emot.core.emot()\n",
    "\n",
    "        # Extract emoticons\n",
    "        emoticon_results = e.emoticons(text)\n",
    "        for original, meaning in zip(emoticon_results['value'], emoticon_results['mean']):\n",
    "            text = text.replace(original, f\" {meaning} \")\n",
    "\n",
    "        # Extract emojis\n",
    "        # emoji_results = e.emoji(text)\n",
    "        # for original, meaning in zip(emoji_results['value'], emoji_results['mean']):\n",
    "        #     text = text.replace(original, f\" {meaning} \")\n",
    "\n",
    "        text = emoji.demojize(text)\n",
    "\n",
    "\n",
    "\n",
    "        return text.strip().lower()\n",
    "        \n",
    "\n",
    "\n",
    "    async def get_posts_byprawn(self, subreddits, limit=1000, mental=\"mental_\"):\n",
    "\n",
    "        reddit = asyncpraw.Reddit(client_id=self.client_id,\n",
    "                             client_secret=self.client_secret,\n",
    "                             user_agent='windows:mentalhealth.scraper:v1.0 (by u/IceWorth5480)',\n",
    "                             username=self.username,\n",
    "                             password=self.password)\n",
    "        \n",
    "        all_posts = []\n",
    "        # , 'top', 'new'\n",
    "        sort_types = ['hot']\n",
    "\n",
    "\n",
    "        for subreddit_name in tqdm(subreddits, desc=\"Subreddits Progress\"):\n",
    "            subreddit = await reddit.subreddit(subreddit_name)\n",
    "            for sort in sort_types:\n",
    "                if sort == 'hot':\n",
    "                    posts = subreddit.hot(limit=limit)\n",
    "                elif sort == 'top':\n",
    "                    posts = subreddit.top(limit=limit)\n",
    "                elif sort == 'new':\n",
    "                    posts = subreddit.new(limit=limit)\n",
    "\n",
    "                async for post in posts:\n",
    "                    if post is None:\n",
    "                        continue\n",
    "\n",
    "                    # Load top-level comments (non-blocking)\n",
    "                    await post.load()\n",
    "                    await post.comments.replace_more(limit=0)\n",
    "                    top_comments_raw = [comment.body for comment in post.comments[:5]]\n",
    "                    top_comments = [self.convert_emojis_emoticons(c) for c in top_comments_raw]\n",
    "\n",
    "                    all_posts.append({\n",
    "                        'id': post.id,\n",
    "                        'subreddit': subreddit_name,\n",
    "                        'sort': sort,\n",
    "                        'title': post.title,\n",
    "                        'selftext': self.convert_emojis_emoticons(post.selftext),\n",
    "                        'created_utc': post.created_utc,\n",
    "                        'score': post.score,\n",
    "                        'num_comments': post.num_comments,\n",
    "                        'author': str(post.author),\n",
    "                        'post_url': post.url,\n",
    "                        'over_18': post.over_18,\n",
    "                        'flair': post.link_flair_text,\n",
    "                        'top_comments': top_comments\n",
    "                    })\n",
    "\n",
    "        await reddit.close()\n",
    "\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(all_posts)\n",
    "        # Drop duplicates by post ID\n",
    "        df = df.drop_duplicates(subset='id').reset_index(drop=True)\n",
    "        df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "        df = df.sort_values(by='created_utc', ascending=False).reset_index(drop=True)\n",
    "        # Convert list of comments to string for CSV storage\n",
    "        df['top_comments'] = df['top_comments'].apply(lambda x: ' | '.join(x) if isinstance(x, list) else '')\n",
    "\n",
    "\n",
    "\n",
    "        # check if f'./data/reddit_data/{mental}reddit_posts.csv' exists, if so merging with df and # delete dupicates by id\n",
    "        if os.path.exists(f'./data/reddit_data/{mental}reddit_posts.csv'):\n",
    "            existing_df = pd.read_csv(f'./data/reddit_data/{mental}reddit_posts.csv')\n",
    "            df = pd.concat([existing_df, df]).drop_duplicates(subset='id').reset_index(drop=True)\n",
    "       \n",
    "        # save as csv \n",
    "        df.to_csv(f'./data/reddit_data/{mental}reddit_posts.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import asyncio\n",
    "\n",
    "\n",
    "\n",
    "# , 'depression', 'anxiety', 'therapy', 'selfhelp', 'bpd', 'ptsd', 'socialanxiety', 'counseling'\n",
    "mental_subreddits = ['mentalhealth', 'depression', 'anxiety', 'therapy', 'selfhelp', 'bpd', 'ptsd', 'socialanxiety', 'counseling']\n",
    "australian_regions = ['melbourne','sydney','adelaide','perth','brisbane','canberra']\n",
    "normal_subreddits = ['popular','all','AskReddit','interestingasfuck']\n",
    "\n",
    "\n",
    "\n",
    "scraper  = RedditScraper()\n",
    "\n",
    "\n",
    "await scraper.get_posts_byprawn(mental_subreddits, limit=1000, mental=\"mental_\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a6a469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93, 13)\n"
     ]
    }
   ],
   "source": [
    "# read the csv file \n",
    "\n",
    "df_mental = pd.read_csv('./data/reddit_data/mental_reddit_posts.csv')\n",
    "print(df_mental.shape)\n",
    "\n",
    "# df_normal = pd.read_csv('./data/reddit_data/normal_reddit_posts.csv')\n",
    "# print(df_normal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5613c",
   "metadata": {},
   "source": [
    "Beyond Blue forums "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "175d8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TO DO\n",
    "\n",
    "Extract following information from the reddit webpage \n",
    "\n",
    "\n",
    "Post ID: A unique identifier for each post.\n",
    "Post Content: The text of the post.\n",
    "Post Author: The author of the post.\n",
    "Post Date: The date the post was made.\n",
    "Post Category: Category or forum where the post was made.\n",
    "Number of Comments: The total number of comments on the post.\n",
    "\n",
    "From Comment \n",
    "\n",
    "Post ID: Link back to the original post.\n",
    "Comment ID: A unique identifier for each comment.\n",
    "Comment Content: Text of the comment.\n",
    "Comment Author: Author of the comment.\n",
    "Comment Date: Date the comment was posted. (the order of the comments is really important)\n",
    "other meta data if available\n",
    "'''\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "import emot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_post_date(raw_date):\n",
    "\n",
    "    weekdays = [day.lower() for day in list(calendar.day_name) ]\n",
    "    today = datetime.now()\n",
    "\n",
    "    raw_date = raw_date.strip().lower()  \n",
    "\n",
    "    # Case 1: Day of the week (e.g., \"Monday\")\n",
    "    if raw_date in weekdays:\n",
    "        # Get weekday index: Monday = 0, Sunday = 6\n",
    "        post_weekday_index = weekdays.index(raw_date)\n",
    "        today_weekday_index = today.weekday()\n",
    "\n",
    "        # Calculate difference in days\n",
    "        delta_days = (today_weekday_index - post_weekday_index) % 7\n",
    "        # Get actual date\n",
    "        post_date = today - timedelta(days=delta_days)\n",
    "        return post_date.strftime('%Y-%m-%d')  # Format as YYYY-MM-DD\n",
    "    \n",
    "    # Case 2: \"a week ago\", \"2 weeks ago\", etc.\n",
    "    elif \"week\" in raw_date:\n",
    "        match = re.search(r'(\\d+)', raw_date)\n",
    "        weeks = int(match.group(1)) if match else 1\n",
    "        post_date = today - timedelta(weeks=weeks)\n",
    "        return post_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Case 3: \"a month ago\", \"2 months ago\", etc.\n",
    "    elif \"month\" in raw_date:\n",
    "        match = re.search(r'(\\d+)', raw_date)\n",
    "        months = int(match.group(1)) if match else 1\n",
    "        # Approximate a month as 30 days\n",
    "        post_date = today - timedelta(days=30 * months)\n",
    "        return post_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Case 4: Exact date format like \"11-05-2025\"\n",
    "    else:\n",
    "        # Try parsing date in the format like \"25-09-2020\"\n",
    "        try:\n",
    "            post_date = datetime.strptime(raw_date, '%d-%m-%Y')\n",
    "            return post_date.strftime('%Y-%m-%d')\n",
    "        except ValueError:\n",
    "            return 'Unknown date'  # If format is unexpected\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_emojis_emoticons(text):\n",
    "    e = emot.core.emot()\n",
    "\n",
    "    # Extract emoticons\n",
    "    emoticon_results = e.emoticons(text)\n",
    "\n",
    "    for original, meaning in zip(emoticon_results['value'], emoticon_results['mean']):\n",
    "        text = text.replace(original, f\" {meaning} \")\n",
    "\n",
    "    # # Extract emojis\n",
    "    # emoji_results = e.emoji(text)\n",
    "    # for original, meaning in zip(emoji_results['value'], emoji_results['mean']):\n",
    "    #     text = text.replace(original, f\" {meaning} \")\n",
    "\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def comment_scrapping (url, comment_pages = 1):\n",
    "\n",
    "    \n",
    "    # Setup Chrome WebDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    comment_driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    list_comments = []\n",
    "\n",
    "    for page in range(1, comment_pages + 1):\n",
    "\n",
    "        comment_driver.get(url)\n",
    "        time.sleep(0.05)\n",
    "        soup = BeautifulSoup(comment_driver.page_source, 'html.parser')\n",
    "        comments_section = soup.find('div', class_='linear-message-list message-list')\n",
    "        every_comments = comments_section.find_all('div')\n",
    "\n",
    "\n",
    "        for comment in every_comments:\n",
    "\n",
    "            main_section = comment.find('div', class_='lia-quilt-row lia-quilt-row-message-main') if comment.find('div', class_='lia-quilt-row lia-quilt-row-message-main') else None\n",
    "            if not main_section:\n",
    "                continue\n",
    "            # scrapping all the text \n",
    "            comment_text = main_section.get_text(separator=' ', strip=True) if main_section else \"\"\n",
    "            comment_text = convert_emojis_emoticons(comment_text)\n",
    "            list_comments.append(comment_text)\n",
    "\n",
    "            # only scrapping the first 3 comments\n",
    "            if len(list_comments) >= 3:\n",
    "                break\n",
    "\n",
    "        # Find next page link\n",
    "        next_page = soup.find('li', class_='lia-paging-page-next')\n",
    "        if next_page and next_page.find('a'):\n",
    "            next_page_link = next_page.find('a')['href']\n",
    "            url = next_page_link\n",
    "        else:\n",
    "            print(\"No more pages to scrape.\")\n",
    "            break\n",
    "        \n",
    "    # close the driver\n",
    "    comment_driver.quit()\n",
    "\n",
    "    # change the list of comments to a string\n",
    "    list_comments = ' | '.join(list_comments) if list_comments else \"\"\n",
    "    \n",
    "\n",
    "    return list_comments\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def beyondblue_scrapping(tag,address,pages=2):\n",
    "\n",
    "\n",
    "    # Setup Chrome WebDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    url = address\n",
    "\n",
    "    whole_data = []\n",
    "\n",
    "\n",
    "\n",
    "    for page in tqdm(range(1, pages + 1), desc=\"Scraping pages\"):\n",
    "\n",
    "    \n",
    "        driver.get(url)\n",
    "        # time.sleep(3)  # Wait for the page to load\n",
    "\n",
    "\n",
    "        # get the class \"custom-message-list all-discussions\"\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "\n",
    "        discussions = soup.find('div', class_='custom-message-list all-discussions').find_all(('article'))\n",
    "        \n",
    "\n",
    "        for post in discussions:\n",
    "\n",
    "            # Extracting post id\n",
    "            post_link = post.find('h3').find_all('a')[1].get('href')\n",
    "            post_id = post_link.split('/')[-1]  \n",
    "\n",
    "\n",
    "            full_post_link = f\"https://forums.beyondblue.org.au{post_link}\"\n",
    "            # extract comments from post_link\n",
    "            comments = comment_scrapping(full_post_link, comment_pages=1)\n",
    "            \n",
    "\n",
    "            # Extracting post title\n",
    "            title_tag = post.find_all('h3')[0].find_all('a')[1]\n",
    "            post_title = convert_emojis_emoticons(title_tag.text.strip()) if title_tag else \"\"\n",
    "\n",
    "            # Extracting post content\n",
    "            post_content = convert_emojis_emoticons(post.find('p', class_ = 'body-text').text.strip()) if post.find('p', class_ = 'body-text') else \"\"\n",
    "            \n",
    "\n",
    "            side_info = post.find('aside')\n",
    "            side_info1 = side_info.find('div', class_='custom-tile-author-info') if side_info else None\n",
    "            # Extracting post author\n",
    "            post_author = side_info1.find('a').find('span').text.strip() if side_info and side_info.find('span') else \"\"\n",
    "\n",
    "            author_link = post.find('aside').find('div', class_='custom-tile-author-info').find('a').get('href')\n",
    "            # Extracting user id from author link\n",
    "            user_id = author_link.split('user-id/')[-1]\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            side_info2 = side_info.find('div', class_='custom-tile-category-content')\n",
    "            # Extracting post tag\n",
    "            post_tag = side_info2.find('a').text.strip() if side_info2 and side_info2.find('a') else \"\"\n",
    "            \n",
    "            \n",
    "            raw_date = side_info2.find('time').text.strip() if side_info2 and side_info2.find('time') else \"\"\n",
    "            # Extracting post date\n",
    "            post_date = parse_post_date(raw_date)\n",
    "\n",
    "\n",
    "\n",
    "            side_info3 = side_info.find('div', class_='custom-tile-unread-replies')\n",
    "            unread = side_info3.find('span').text.strip() if side_info3 and side_info3.find('span') else \"\"\n",
    "            # Extracting number of unread replies\n",
    "            match = re.search(r'\\d+', unread)\n",
    "            post_unread = int(match.group()) if match else 0\n",
    "\n",
    "            # Extracting number of comments\n",
    "            number_comments = post.find('li', class_ = 'custom-tile-replies').find('b').text.strip() if post.find('li', class_ = 'custom-tile-replies') else \"\"\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            post_data = {\n",
    "                \"Post ID\": post_id,\n",
    "                \"Post Title\": post_title,\n",
    "                \"Post Content\": post_content,\n",
    "                \"Post Author\": post_author,\n",
    "                \"User ID\": user_id,\n",
    "                \"Post Date\": post_date,\n",
    "                \"Post Category\": post_tag,\n",
    "                \"Number of Comments\": number_comments,\n",
    "                \"Comments\": comments\n",
    "            }\n",
    "            whole_data.append(post_data)\n",
    "\n",
    "\n",
    "        # Print the number of posts scraped on the current page\n",
    "        # print(f\"Tag: {tag} Page {page}: Scraped {len(discussions)} posts.\")\n",
    "\n",
    "        # Find next page link\n",
    "        next_page = soup.find('li', class_='lia-paging-page-next')\n",
    "        if next_page and next_page.find('a'):\n",
    "            url = next_page.find('a')['href']\n",
    "        else:\n",
    "            print(\"No more pages to scrape.\")\n",
    "            break\n",
    "\n",
    "    #close the driver\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "    # Convert to DataFrame \n",
    "    df = pd.DataFrame(whole_data)\n",
    "    # delete duplicates by Post ID\n",
    "    df = df.drop_duplicates(subset='Post ID').reset_index(drop=True)\n",
    "    # Convert 'Post Date' to datetime format\n",
    "    df['Post Date'] = pd.to_datetime(df['Post Date'], errors='coerce')\n",
    "    # Sort by 'Post Date' in descending order\n",
    "    df = df.sort_values(by='Post Date', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # check if the file already exists\n",
    "    if os.path.exists(f'./data/beyondblue_data/{tag}_beyondblue_posts.csv'):\n",
    "        existing_df = pd.read_csv(f'./data/beyondblue_data/{tag}_beyondblue_posts.csv')\n",
    "        df = pd.concat([existing_df, df]).drop_duplicates(subset='Post ID').reset_index(drop=True)\n",
    "\n",
    "    df.to_csv(f'./data/beyondblue_data/{tag}_beyondblue_posts.csv', index=False)\n",
    "        \n",
    "        \n",
    "    print(f\"Data saved to ./data/beyondblue_data/{tag}_beyondblue_posts.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df711d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 199/199 [04:25<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/beyondblue_data/Anxiety_beyondblue_posts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 199/199 [05:36<00:00,  1.69s/it]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6060\\3116285400.py:202: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Post Date'] = pd.to_datetime(df['Post Date'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/beyondblue_data/Depression_beyondblue_posts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 199/199 [05:34<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/beyondblue_data/PTSD_beyondblue_posts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  65%|██████▌   | 130/199 [03:38<01:55,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages to scrape.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/beyondblue_data/Suicidal_thoughts_and_self-harm_beyondblue_posts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 199/199 [05:27<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/beyondblue_data/Relationship and family issues_beyondblue_posts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 199/199 [04:34<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/beyondblue_data/Young people_beyondblue_posts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  39%|███▊      | 77/199 [02:13<03:31,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages to scrape.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6060\\3116285400.py:202: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['Post Date'] = pd.to_datetime(df['Post Date'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/beyondblue_data/Sexuality and gender identity_beyondblue_posts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  13%|█▎        | 25/199 [00:46<05:21,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages to scrape.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/beyondblue_data/Multicultural experiences_beyondblue_posts.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages:  44%|████▎     | 87/199 [02:29<03:12,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more pages to scrape.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/beyondblue_data/Grief and loss_beyondblue_posts.csv\n"
     ]
    }
   ],
   "source": [
    "# post_variables = [\"Post ID\", \"Post Content\", \"Post Author\", \"Post Date\", \"Post Category\", \"Number of Comments\"]\n",
    "# mental_health_type =['Anxiety','Depression','PTSD and trauma','Suicidal thoughts and self-harm']\n",
    "\n",
    "\n",
    "\n",
    "mental_health_urls = {\n",
    "    \"Anxiety\": \"https://forums.beyondblue.org.au/t5/anxiety/bd-p/c1-sc2-b1?&sort=kudos\",\n",
    "    \"Depression\": \"https://forums.beyondblue.org.au/t5/depression/bd-p/c1-sc2-b2?&sort=kudos\",\n",
    "    \"PTSD\": \"https://forums.beyondblue.org.au/t5/ptsd-and-trauma/bd-p/c1-sc2-b3?&sort=kudos\",\n",
    "    \"Suicidal_thoughts_and_self-harm\": \"https://forums.beyondblue.org.au/t5/suicidal-thoughts-and-self-harm/bd-p/c1-sc2-b4?&sort=kudos\",\n",
    "    \"Relationship and family issues\":\"https://forums.beyondblue.org.au/t5/relationship-and-family-issues/bd-p/c1-sc3-b3?&sort=kudos\",\n",
    "    \"Young people\":\"https://forums.beyondblue.org.au/t5/young-people/bd-p/c1-sc4-b1?&sort=kudos\",\n",
    "    \"Sexuality and gender identity\":\"https://forums.beyondblue.org.au/t5/sexuality-and-gender-identity/bd-p/c1-sc4-b2?&sort=kudos\",\n",
    "    \"Multicultural experiences\":\"https://forums.beyondblue.org.au/t5/multicultural-experiences/bd-p/c1-sc4-b3?&sort=kudos\",\n",
    "    \"Grief and loss\":\"https://forums.beyondblue.org.au/t5/grief-and-loss/bd-p/c1-sc4-b4?&sort=kudos\",\n",
    "}\n",
    "\n",
    "each_data_number = 2000\n",
    "pages = each_data_number // 10  # Assuming each page has 10 posts\n",
    "\n",
    "\n",
    "for tag, address in mental_health_urls.items():\n",
    "    try:\n",
    "        beyondblue_scrapping(tag, address, pages = pages)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {tag}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e68d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1990, 8)\n",
      "      Post ID                          Post Title  \\\n",
      "0      611174                Peripheral Neuropahy   \n",
      "1      611132                 Apprentice mechanic   \n",
      "2      611126              Depression and Anxiety   \n",
      "3      611099  Easy strategies for quick response   \n",
      "4      611075                                Help   \n",
      "...       ...                                 ...   \n",
      "1985    56941          Scared to death, OF DEATH.   \n",
      "1986    38359         Severe paranoia and anxiety   \n",
      "1987    37072                      Health Anxiety   \n",
      "1988    37038                  Feeling very angry   \n",
      "1989    94973              Running out of excuses   \n",
      "\n",
      "                                           Post Content     Post Author  \\\n",
      "0     Has anyone has success with HBOT Therapy for P...  Guest_55166016   \n",
      "1     I'm 4 months in to my apprenticeship and em st...  Guest_28908038   \n",
      "2     I am currently an international student. I alr...  Guest_16677420   \n",
      "3     Recently my anxiety has 'flared'. As soon as I...         Olive83   \n",
      "4     Just need help processing things, been diagnos...  Guest_91699169   \n",
      "...                                                 ...             ...   \n",
      "1985  To all the fellow sufferers of anxiety, Ever s...         Chin_Up   \n",
      "1986  Hi all, I'm quite new to this so forgive me if...             Bes   \n",
      "1987  Hi there, I heard Beyond Blue advertised on th...       Buckley05   \n",
      "1988  I have just read that Jeff Kennett thinks that...         Maxwell   \n",
      "1989  I'm 45 and running out of excuses for not atte...           lidia   \n",
      "\n",
      "      User ID   Post Date Post Category  Number of Comments  \n",
      "0       55404  2025-06-11       Anxiety                   0  \n",
      "1       55389  2025-06-10       Anxiety                   3  \n",
      "2       55388  2025-06-10       Anxiety                   1  \n",
      "3       55379  2025-06-09       Anxiety                   3  \n",
      "4       55371  2025-06-08       Anxiety                   1  \n",
      "...       ...         ...           ...                 ...  \n",
      "1985    13511  2013-06-17       Anxiety                  35  \n",
      "1986    37963  2013-05-09       Anxiety                   4  \n",
      "1987    13331  2013-05-06       Anxiety                   1  \n",
      "1988     8120  2013-05-06       Anxiety                   2  \n",
      "1989    19542  2013-04-06       Anxiety                   5  \n",
      "\n",
      "[1990 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read the csv file\n",
    "df_anxiety = pd.read_csv('./data/beyondblue_data/Anxiety_beyondblue_posts.csv')\n",
    "df_depression = pd.read_csv('./data/beyondblue_data/Depression_beyondblue_posts.csv')\n",
    "df_ptsd = pd.read_csv('./data/beyondblue_data/PTSD_beyondblue_posts.csv')\n",
    "df_self_harm = pd.read_csv('./data/beyondblue_data/Suicidal_thoughts_and_self-harm_beyondblue_posts.csv')\n",
    "\n",
    "\n",
    "\n",
    "print(df_anxiety.shape)\n",
    "# print(df_anxiety)\n",
    "\n",
    "print(df_depression.shape)  \n",
    "print(df_ptsd.shape)\n",
    "print(df_self_harm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf93aa",
   "metadata": {},
   "source": [
    "Climate data API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c274975",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53210b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78036975",
   "metadata": {},
   "source": [
    "# Nature Language Process\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
